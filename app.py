# -*- coding: utf-8 -*-
"""
V116.18 å°è‚¡æ³¨æ„è‚¡ç³»çµ± (Zeabur Stable Version - Simple Request with Delay)
ä¿®æ­£é‡é»ï¼š
1. [æ ¸å¿ƒ] ç§»é™¤è¤‡é›œ Sessionï¼Œå›æ­¸å–®ç´” requests + å¼·åˆ¶å»¶é² (é¿å…è¢«æ“‹)ã€‚
2. [é¡¯ç¤º] ä¿®æ­£ã€Œè™•ç½®ä¸­ã€çš„é¡¯ç¤ºé‚è¼¯ï¼šæœ€å¿«è™•ç½®å¤©æ•¸é¡¯ç¤º "0"ã€‚
3. [é‚è¼¯] ä¿®æ­£ã€Œå·²é”æ¨™ã€å¤©æ•¸ï¼šç•¶æ—¥é”æ¨™è¦–åŒé€²å…¥è™•ç½®ï¼ˆ0å¤©ï¼‰ã€‚
4. [Zeabur] é©é…ç’°å¢ƒè®Šæ•¸èˆ‡ SSL å¿½ç•¥ã€‚
"""

import os
import sys
import time  # å¼•å…¥ time ç”¨æ–¼å»¶é²

# è‡ªå‹•å®‰è£ç¼ºå°‘çš„å¥—ä»¶
try:
    import twstock
    import yfinance as yf
    import pandas as pd
    import numpy as np
    import requests
    import re
    import gspread
    import logging
    import urllib3
    from google.oauth2.service_account import Credentials
    from google.auth import default
    from datetime import datetime, timedelta, time as dt_time, date
    from dateutil.relativedelta import relativedelta
    from zoneinfo import ZoneInfo
except ImportError:
    os.system('pip install twstock yfinance gspread google-auth python-dateutil requests pandas zoneinfo --quiet')
    import twstock
    import yfinance as yf
    import pandas as pd
    import numpy as np
    import requests
    import re
    import gspread
    import logging
    import urllib3
    from google.oauth2.service_account import Credentials
    from google.auth import default
    from datetime import datetime, timedelta, time as dt_time, date
    from dateutil.relativedelta import relativedelta
    from zoneinfo import ZoneInfo

# ==========================================
# 1. è¨­å®šéœéŸ³æ¨¡å¼èˆ‡å¸¸æ•¸
# ==========================================
# å¿½ç•¥ SSL è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

logger = logging.getLogger('yfinance')
logger.setLevel(logging.CRITICAL)
logger.disabled = True

UNIT_LOT = 1000

# å®šç¾©çµ±è¨ˆè¡¨é ­
STATS_HEADERS = [
    'ä»£è™Ÿ', 'åç¨±', 'é€£çºŒå¤©æ•¸', 'è¿‘30æ—¥æ³¨æ„æ¬¡æ•¸', 'è¿‘10æ—¥æ³¨æ„æ¬¡æ•¸', 'æœ€è¿‘ä¸€æ¬¡æ—¥æœŸ',
    '30æ—¥ç‹€æ…‹ç¢¼', '10æ—¥ç‹€æ…‹ç¢¼', 'æœ€å¿«è™•ç½®å¤©æ•¸', 'è™•ç½®è§¸ç™¼åŸå› ', 'é¢¨éšªç­‰ç´š', 'è§¸ç™¼æ¢ä»¶',
    'ç›®å‰åƒ¹', 'è­¦æˆ’åƒ¹', 'å·®å¹…(%)', 'ç›®å‰é‡', 'è­¦æˆ’é‡', 'æˆäº¤å€¼(å„„)',
    'é€±è½‰ç‡(%)', 'PE', 'PB', 'ç•¶æ²–ä½”æ¯”(%)'
]

# ==========================================
# ğŸ“† è¨­å®šå€
# ==========================================
SHEET_NAME = "å°è‚¡æ³¨æ„è‚¡è³‡æ–™åº«_V33"
PARAM_SHEET_NAME = "å€‹è‚¡åƒæ•¸"
# Zeabur é è¨­æ™‚å€å¯èƒ½ç‚º UTCï¼Œå¼·åˆ¶æŒ‡å®šå°åŒ—æ™‚é–“
try:
    TW_TZ = ZoneInfo("Asia/Taipei")
except:
    TW_TZ = ZoneInfo("UTC") # Fallback

TARGET_DATE = datetime.now(TW_TZ)

SAFE_CRAWL_TIME = dt_time(19, 0)
SAFE_MARKET_OPEN_CHECK = dt_time(16, 30)

# ==========================================
# ğŸ”‘ FinMind é‡‘é‘°è¨­å®š
# ==========================================
FINMIND_API_URL = "https://api.finmindtrade.com/api/v4/data"

FINMIND_TOKENS = []
# 1. å˜—è©¦è®€å–ç’°å¢ƒè®Šæ•¸ (Zeabur)
env_token = os.getenv('FinMind_1')
if env_token: FINMIND_TOKENS.append(env_token)
env_token2 = os.getenv('FinMind_2')
if env_token2: FINMIND_TOKENS.append(env_token2)

# 2. å˜—è©¦è®€å– Colab userdata (Fallback)
try:
    from google.colab import userdata
    t1 = userdata.get('FinMind_1')
    if t1 and t1 not in FINMIND_TOKENS: FINMIND_TOKENS.append(t1)
    t2 = userdata.get('FinMind_2')
    if t2 and t2 not in FINMIND_TOKENS: FINMIND_TOKENS.append(t2)
except: pass

CURRENT_TOKEN_INDEX = 0
_FINMIND_CACHE = {}

print(f"ğŸš€ å•Ÿå‹• V116.18 å°è‚¡æ³¨æ„è‚¡ç³»çµ± (Zeabur Stable)")
print(f"ğŸ•’ ç³»çµ±æ™‚é–“ (Taiwan): {TARGET_DATE.strftime('%Y-%m-%d %H:%M:%S')}")

try: twstock.__update_codes()
except: pass

# ============================
# ğŸ› ï¸ å·¥å…·å‡½å¼
# ============================
CN_NUM = {"ä¸€":"1","äºŒ":"2","ä¸‰":"3","å››":"4","äº”":"5","å…­":"6","ä¸ƒ":"7","å…«":"8","ä¹":"9","å":"10"}

KEYWORD_MAP = {
    "èµ·è¿„å…©å€‹ç‡Ÿæ¥­æ—¥": 11, "ç•¶æ—¥æ²–éŠ·": 13, "å€Ÿåˆ¸è³£å‡º": 12, "ç´¯ç©é€±è½‰ç‡": 10, "é€±è½‰ç‡": 4,
    "æˆäº¤é‡": 9, "æœ¬ç›Šæ¯”": 6, "è‚¡åƒ¹æ·¨å€¼æ¯”": 6, "æº¢æŠ˜åƒ¹": 8, "æ”¶ç›¤åƒ¹æ¼²è·Œç™¾åˆ†æ¯”": 1,
    "æœ€å¾Œæˆäº¤åƒ¹æ¼²è·Œ": 1, "æœ€è¿‘å…­å€‹ç‡Ÿæ¥­æ—¥ç´¯ç©": 1
}

def normalize_clause_text(s: str) -> str:
    if not s: return ""
    s = str(s)
    s = s.replace("ç¬¬ã„§æ¬¾", "ç¬¬ä¸€æ¬¾")
    for cn, dg in CN_NUM.items():
        s = s.replace(f"ç¬¬{cn}æ¬¾", f"ç¬¬{dg}æ¬¾")
    s = s.translate(str.maketrans("ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼", "1234567890"))
    return s

def parse_clause_ids_strict(clause_text):
    if not isinstance(clause_text, str): return set()
    clause_text = normalize_clause_text(clause_text)
    ids = set()
    matches = re.findall(r'ç¬¬\s*(\d+)\s*æ¬¾', clause_text)
    for m in matches: ids.add(int(m))
    if not ids:
        for keyword, code in KEYWORD_MAP.items():
            if keyword in clause_text: ids.add(code)
    return ids

def merge_clause_text(a, b):
    ids = set()
    ids |= parse_clause_ids_strict(a) if a else set()
    ids |= parse_clause_ids_strict(b) if b else set()
    if ids: return "ã€".join([f"ç¬¬{x}æ¬¾" for x in sorted(ids)])
    a = a or ""; b = b or ""
    return a if len(a) >= len(b) else b

def is_valid_accumulation_day(ids):
    if not ids: return False
    return any(1 <= x <= 8 for x in ids)

def is_special_risk_day(ids):
    if not ids: return False
    return any(9 <= x <= 14 for x in ids)

def get_ticker_suffix(market_type):
    m = str(market_type).upper().strip()
    keywords = ['ä¸Šæ«ƒ', 'TWO', 'TPEX', 'OTC']
    if any(k in m for k in keywords): return '.TWO'
    return '.TW'

def get_or_create_ws(sh, title, headers=None, rows=5000, cols=20):
    need_cols = max(cols, len(headers) if headers else 0)
    try:
        ws = sh.worksheet(title)
        try:
            if headers and ws.col_count < need_cols: ws.resize(rows=ws.row_count, cols=need_cols)
        except: pass
        return ws
    except:
        print(f"âš ï¸ å·¥ä½œè¡¨ '{title}' ä¸å­˜åœ¨ï¼Œæ­£åœ¨å»ºç«‹...")
        ws = sh.add_worksheet(title=title, rows=str(rows), cols=str(need_cols))
        if headers: ws.append_row(headers, value_input_option="USER_ENTERED")
        return ws

# ============================
# API å·¥å…·å‡½æ•¸ (å«å»¶é²èˆ‡é‡è©¦)
# ============================
def finmind_get(dataset, data_id=None, start_date=None, end_date=None):
    global CURRENT_TOKEN_INDEX
    cache_key = (dataset, data_id, start_date, end_date)
    if cache_key in _FINMIND_CACHE: return _FINMIND_CACHE[cache_key].copy()

    params = {"dataset": dataset}
    if data_id: params["data_id"] = str(data_id)
    if start_date: params["start_date"] = start_date
    if end_date: params["end_date"] = end_date
    
    tokens_to_try = FINMIND_TOKENS if FINMIND_TOKENS else [None]

    for _ in range(4):
        # [Fix] æ¯æ¬¡è«‹æ±‚å‰å¼·åˆ¶å»¶é² 1 ç§’
        time.sleep(1)
        
        token = tokens_to_try[CURRENT_TOKEN_INDEX % len(tokens_to_try)]
        headers = {"User-Agent": "Mozilla/5.0", "Connection": "close"}
        if token: headers["Authorization"] = f"Bearer {token}"
            
        try:
            r = requests.get(FINMIND_API_URL, params=params, headers=headers, timeout=10, verify=False)
            if r.status_code == 200:
                j = r.json()
                df = pd.DataFrame(j["data"]) if "data" in j else pd.DataFrame()
                if len(_FINMIND_CACHE) >= 2000: _FINMIND_CACHE.clear()
                _FINMIND_CACHE[cache_key] = df
                return df.copy()
            elif r.status_code != 200 and token:
                print(f"   âš ï¸ Token {CURRENT_TOKEN_INDEX} ç•°å¸¸ï¼Œåˆ‡æ›ä¸‹ä¸€çµ„...")
                time.sleep(2)
                CURRENT_TOKEN_INDEX += 1
                continue
        except: time.sleep(1)
    return pd.DataFrame()

# ============================
# å¤§ç›¤ç›£æ§æ›´æ–°
# ============================
def update_market_monitoring_log(sh):
    print("ğŸ“Š æª¢æŸ¥ä¸¦æ›´æ–°ã€Œå¤§ç›¤æ•¸æ“šç›£æ§ã€...")
    HEADERS = ['æ—¥æœŸ', 'ä»£è™Ÿ', 'åç¨±', 'æ”¶ç›¤åƒ¹', 'æ¼²è·Œå¹…(%)', 'æˆäº¤é‡‘é¡(å„„)']
    ws_market = get_or_create_ws(sh, "å¤§ç›¤æ•¸æ“šç›£æ§", headers=HEADERS, cols=10)

    def norm_date(s):
        s = str(s).strip()
        if not s: return ""
        try: return pd.to_datetime(s, errors='coerce').strftime("%Y-%m-%d")
        except: return s

    key_to_row = {}
    try:
        all_vals = ws_market.get_all_values()
        for r_idx, row in enumerate(all_vals[1:], start=2):
            if len(row) >= 2:
                d_str = norm_date(row[0])
                c_str = str(row[1]).strip()
                if d_str and c_str: key_to_row[f"{d_str}_{c_str}"] = r_idx
    except: pass

    existing_keys = set(key_to_row.keys())

    try:
        targets = [
            {'fin_id': 'TAIEX', 'code': '^TWII', 'name': 'åŠ æ¬ŠæŒ‡æ•¸'},
            {'fin_id': 'TPEx',  'code': '^TWOII', 'name': 'æ«ƒè²·æŒ‡æ•¸'}
        ]
        start_date_str = (TARGET_DATE - timedelta(days=45)).strftime("%Y-%m-%d")

        dfs = {}
        for t in targets:
            df = finmind_get("TaiwanStockPrice", data_id=t['fin_id'], start_date=start_date_str)
            if not df.empty:
                if 'date' in df.columns:
                    df['date'] = pd.to_datetime(df['date'])
                    df.set_index('date', inplace=True)
                    df.index = df.index.tz_localize(None)
                if 'close' in df.columns:
                    df['Close'] = df['close'].astype(float)
                    df['Pct'] = df['Close'].pct_change() * 100
                if 'Turnover' in df.columns: df['Volume'] = df['Turnover'].astype(float)
                elif 'Trading_money' in df.columns: df['Volume'] = df['Trading_money'].astype(float)
                else: df['Volume'] = 0.0
                dfs[t['code']] = df

        new_rows = []
        today_str = TARGET_DATE.strftime("%Y-%m-%d")
        all_dates = set()
        for df in dfs.values(): all_dates.update(df.index.strftime("%Y-%m-%d").tolist())

        for d in sorted(all_dates):
            for t in targets:
                code = t['code']
                df = dfs.get(code)
                if df is None or d not in df.index.strftime("%Y-%m-%d"): continue
                try: row = df.loc[d]
                except: row = df[df.index.strftime("%Y-%m-%d") == d].iloc[0]
                close_val = row.get('Close', 0)
                if pd.isna(close_val): continue
                close = round(float(close_val), 2)
                pct = round(float(row.get('Pct', 0) or 0), 2)
                vol_raw = float(row.get('Volume', 0) or 0)
                vol_billion = round(vol_raw / 100000000, 2)
                row_data = [d, code, t['name'], close, pct, vol_billion]
                comp_key = f"{d}_{code}"

                if d == today_str and TARGET_DATE.time() < SAFE_MARKET_OPEN_CHECK:
                    if code == '^TWII': print(f"   â³ ä»Šæ—¥ ({d}) å°šæœªæ”¶ç›¤ï¼Œè·³éå¯«å…¥ã€‚")
                    continue

                if d == today_str and comp_key in key_to_row and TARGET_DATE.time() >= SAFE_MARKET_OPEN_CHECK:
                    r_num = key_to_row[comp_key]
                    try:
                        ws_market.update(values=[row_data], range_name=f'A{r_num}:F{r_num}', value_input_option="USER_ENTERED")
                        print(f"   ğŸ”„ å·²è¦†å¯«æ›´æ–°ä»Šæ—¥ ({d} {t['name']}) æ•¸æ“š (Row {r_num})ã€‚")
                    except Exception as e:
                        print(f"   âš ï¸ è¦†å¯«å¤±æ•— ({comp_key}): {e}")
                    continue

                if comp_key in existing_keys: continue
                if close > 0: new_rows.append(row_data)

        if new_rows:
            ws_market.append_rows(new_rows, value_input_option="USER_ENTERED")
            print(f"   âœ… å·²è£œå…¥ {len(new_rows)} ç­†å¤§ç›¤æ•¸æ“šã€‚")
        else:
            print("   âœ… å¤§ç›¤æ•¸æ“šå·²æ˜¯æœ€æ–°ï¼Œç„¡éœ€æ–°å¢ã€‚")
    except Exception as e:
        print(f"   âŒ å¤§ç›¤æ•¸æ“šæ›´æ–°å¤±æ•—: {e}")

# ============================
# ğŸ”¥ è™•ç½®è³‡æ–™æŠ“å– (Jail) - å« Zeabur SSL Fix
# ============================
def parse_roc_date(roc_date_str):
    try:
        roc_date_str = str(roc_date_str).strip()
        parts = re.split(r'[/-]', roc_date_str)
        if len(parts) == 3:
            year = int(parts[0]) + 1911
            month = int(parts[1])
            day = int(parts[2])
            return date(year, month, day)
    except: return None
    return None

def parse_jail_period(period_str):
    if not period_str: return None, None
    dates = []
    if 'ï½' in period_str: dates = period_str.split('ï½')
    elif '~' in period_str: dates = period_str.split('~')
    elif '-' in period_str and '/' in period_str:
        if period_str.count('-') == 1: dates = period_str.split('-')
    
    if len(dates) >= 2:
        start_date = parse_roc_date(dates[0].strip())
        end_date = parse_roc_date(dates[1].strip())
        if start_date and end_date: return start_date, end_date
    return None, None

def get_jail_map(start_date_obj, end_date_obj):
    print("ğŸ”’ æ­£åœ¨ä¸‹è¼‰è™•ç½®(Jail)åå–®ä»¥å»ºç«‹æ¿¾ç¶²...")
    jail_map = {}
    s_str = start_date_obj.strftime("%Y%m%d")
    e_str = end_date_obj.strftime("%Y%m%d")

    # 1) TWSE (Listing)
    try:
        # [Fix] å»¶é²é¿å…å°é–
        time.sleep(1)
        url = "https://www.twse.com.tw/rwd/zh/announcement/punish"
        r = requests.get(url, params={"startDate": s_str, "endDate": e_str, "response": "json"}, timeout=10, verify=False)
        j = r.json()
        if isinstance(j.get("tables"), list) and j["tables"]:
            data_rows = j["tables"][0].get("data", [])
            for row in data_rows:
                try:
                    code = str(row[2]).strip()
                    sd, ed = parse_jail_period(str(row[6]).strip())
                    if sd and ed: jail_map.setdefault(code, []).append((sd, ed))
                except: continue
        else:
            for row in j.get("data", []):
                try:
                    code = str(row[2]).strip()
                    sd, ed = parse_jail_period(str(row[6]).strip())
                    if sd and ed: jail_map.setdefault(code, []).append((sd, ed))
                except: continue
    except Exception as e:
        print(f"âš ï¸ TWSE è™•ç½®æŠ“å–å¤±æ•—: {e}")

    # 2) TPEx (OTC) - OpenAPI
    try:
        time.sleep(1)
        url = "https://www.tpex.org.tw/openapi/v1/tpex_disposal_information"
        r = requests.get(url, timeout=10, verify=False)
        if r.status_code == 200:
            data = r.json()
            for item in data:
                try:
                    code = str(item.get("SecuritiesCompanyCode", "")).strip()
                    if len(code) != 4: continue
                    sd, ed = parse_jail_period(item.get("DispositionPeriod", ""))
                    if sd and ed:
                        if ed >= start_date_obj and sd <= end_date_obj:
                            jail_map.setdefault(code, []).append((sd, ed))
                except: continue
    except Exception as e:
        print(f"âš ï¸ TPEx è™•ç½®æŠ“å–å¤±æ•—: {e}")

    for k in jail_map: jail_map[k] = sorted(jail_map[k], key=lambda x: x[0])
    return jail_map

def is_in_jail(stock_id, target_date, jail_map):
    if not jail_map or stock_id not in jail_map: return False
    for start, end in jail_map[stock_id]:
        if start <= target_date <= end: return True
    return False

def prev_trade_date(d, cal_dates):
    if not cal_dates: return None
    try: idx = cal_dates.index(d)
    except:
        for i in range(len(cal_dates)-1, -1, -1):
            if cal_dates[i] < d: return cal_dates[i]
        return None
    if idx - 1 >= 0: return cal_dates[idx - 1]
    return None

def build_exclude_map(cal_dates, jail_map):
    exclude_map = {}
    if not jail_map: return exclude_map
    for code, periods in jail_map.items():
        s = set()
        for start, end in periods:
            pd = prev_trade_date(start, cal_dates)
            if pd: s.add(pd)
            for d in cal_dates:
                if start <= d <= end: s.add(d)
        exclude_map[code] = s
    return exclude_map

def is_excluded(code, d, exclude_map):
    return bool(exclude_map) and (code in exclude_map) and (d in exclude_map[code])

def get_last_n_non_jail_trade_dates(stock_id, cal_dates, jail_map, exclude_map=None, n=30):
    last_jail_end = date(1900, 1, 1)
    if jail_map and stock_id in jail_map:
        last_jail_end = jail_map[stock_id][-1][1]
    picked = []
    for d in reversed(cal_dates):
        if d <= last_jail_end: break
        if is_excluded(stock_id, d, exclude_map): continue
        if jail_map and is_in_jail(stock_id, d, jail_map): continue
        picked.append(d)
        if len(picked) >= n: break
    return list(reversed(picked))

# ============================
# ğŸ”¥ å®˜æ–¹å…¬å‘Šçˆ¬èŸ² (æ³¨æ„è‚¡) - å« Zeabur SSL Fix
# ============================
def get_daily_data(date_obj):
    date_str_nodash = date_obj.strftime("%Y%m%d")
    date_str = date_obj.strftime("%Y-%m-%d")
    rows = []
    error_count = 0

    print(f"ğŸ“¡ å˜—è©¦çˆ¬å–å®˜æ–¹å…¬å‘Š (æ—¥æœŸ: {date_str})...")

    # 1. TWSE
    try:
        # [Fix] å»¶é²
        time.sleep(1)
        r = requests.get("https://www.twse.com.tw/rwd/zh/announcement/notice",
                         params={"startDate": date_str_nodash, "endDate": date_str_nodash, "response": "json"}, 
                         timeout=10, verify=False)
        if r.status_code == 200:
            d = r.json()
            if 'data' in d:
                for i in d['data']:
                    code = str(i[1]).strip(); name = str(i[2]).strip()
                    if not (code.isdigit() and len(code) == 4): continue
                    raw_text = " ".join([str(x) for x in i])
                    ids = parse_clause_ids_strict(raw_text)
                    clause_str = "ã€".join([f"ç¬¬{k}æ¬¾" for k in sorted(ids)])
                    if not clause_str: clause_str = raw_text
                    rows.append({'æ—¥æœŸ': date_str, 'å¸‚å ´': 'TWSE', 'ä»£è™Ÿ': code, 'åç¨±': name, 'è§¸çŠ¯æ¢æ¬¾': clause_str})
        else: error_count += 1
    except: error_count += 1

    # 2. TPEx
    try:
        time.sleep(1)
        roc_date = f"{date_obj.year-1911}/{date_obj.month:02d}/{date_obj.day:02d}"
        headers = {'User-Agent': 'Mozilla/5.0', 'Referer': 'https://www.tpex.org.tw/'}
        r = requests.post("https://www.tpex.org.tw/www/zh-tw/bulletin/attention", 
                          data={'date': roc_date, 'response': 'json'}, 
                          headers=headers, timeout=10, verify=False)
        if r.status_code == 200:
            res = r.json()
            target = []
            if 'tables' in res:
                 for t in res['tables']: target.extend(t.get('data', []))
            elif 'data' in res: target = res['data']
            
            filtered_target = []
            if target:
                for row in target:
                    if len(row) > 5:
                        row_date = str(row[5]).strip()
                        if row_date == roc_date or row_date == date_str:
                            filtered_target.append(row)
            target = filtered_target

            for i in target:
                code = str(i[1]).strip(); name = str(i[2]).strip()
                if not (code.isdigit() and len(code) == 4): continue
                raw_text = " ".join([str(x) for x in i])
                ids = parse_clause_ids_strict(raw_text)
                clause_str = "ã€".join([f"ç¬¬{k}æ¬¾" for k in sorted(ids)])
                if not clause_str: clause_str = raw_text
                rows.append({'æ—¥æœŸ': date_str, 'å¸‚å ´': 'TPEx', 'ä»£è™Ÿ': code, 'åç¨±': name, 'è§¸çŠ¯æ¢æ¬¾': clause_str})
        else: error_count += 1
    except: error_count += 1

    if error_count >= 2 and not rows: return None
    if rows: print(f"âœ… æˆåŠŸæŠ“åˆ° {len(rows)} æª”æ³¨æ„è‚¡ã€‚")
    else: print(f"âš ï¸ è©²æ—¥ ({date_str}) æŸ¥ç„¡è³‡æ–™ã€‚")
    return rows

# ============================
# ğŸ“† äº¤æ˜“æ—¥æ›†
# ============================
def is_market_open_by_finmind(date_str):
    df = finmind_get("TaiwanStockPrice", data_id="2330", start_date=date_str, end_date=date_str)
    return not df.empty

def get_official_trading_calendar(days=60):
    end_str = TARGET_DATE.strftime("%Y-%m-%d")
    start_str = (TARGET_DATE - timedelta(days=days*2)).strftime("%Y-%m-%d")
    print("ğŸ“… æ­£åœ¨ä¸‹è¼‰å®˜æ–¹äº¤æ˜“æ—¥æ›†...")
    df = finmind_get("TaiwanStockTradingDate", start_date=start_str, end_date=end_str)
    dates = []
    if not df.empty:
        df['date'] = pd.to_datetime(df['date']).dt.date
        dates = sorted(df['date'].tolist())
    else:
        curr = TARGET_DATE.date()
        while len(dates) < days:
            if curr.weekday() < 5: dates.append(curr)
            curr -= timedelta(days=1)
        dates = sorted(dates)

    today_date = TARGET_DATE.date()
    today_str = today_date.strftime("%Y-%m-%d")
    if dates and today_date > dates[-1] and today_date.weekday() < 5:
        if TARGET_DATE.time() > SAFE_MARKET_OPEN_CHECK:
            print(f"âš ï¸ é©—è­‰ä»Šæ—¥ ({today_date}) é–‹å¸‚ä¸­...")
            if is_market_open_by_finmind(today_str):
                print("âœ… é©—è­‰æˆåŠŸ (2330æœ‰åƒ¹)ï¼Œè£œå…¥ä»Šæ—¥ã€‚")
                dates.append(today_date)
            else: print("â›” é©—è­‰å¤±æ•— (2330ç„¡åƒ¹)ï¼Œä¸è£œå…¥ã€‚")
        else: print("â³ æ™‚é–“å°šæ—©ï¼Œæš«ä¸å¼·åˆ¶è£œå…¥ã€‚")
    return dates[-days:]

def get_daytrade_stats_finmind(stock_id, target_date_str):
    end_date = target_date_str
    start_date = (datetime.strptime(target_date_str, "%Y-%m-%d") - timedelta(days=15)).strftime("%Y-%m-%d")
    p = finmind_get("TaiwanStockPrice", data_id=stock_id, start_date=start_date, end_date=end_date)
    d = finmind_get("TaiwanStockDayTrading", data_id=stock_id, start_date=start_date, end_date=end_date)
    if p.empty or d.empty: return 0.0, 0.0
    try:
        merged = pd.merge(p[['date', 'Trading_Volume']], d[['date', 'Volume']], on='date', how='inner')
        if merged.empty: return 0.0, 0.0
        merged['date'] = pd.to_datetime(merged['date'])
        merged = merged.sort_values('date')
        r6 = merged.tail(6)
        if len(r6) < 6: return 0.0, 0.0
        last = r6.iloc[-1]
        today = (last['Volume'] / last['Trading_Volume'] * 100.0) if last['Trading_Volume'] > 0 else 0.0
        avg6 = (r6['Volume'].sum() / r6['Trading_Volume'].sum() * 100.0) if r6['Trading_Volume'].sum() > 0 else 0.0
        return round(today, 2), round(avg6, 2)
    except: return 0.0, 0.0

# ============================
# åŸºç¤è³‡æ–™
# ============================
def fetch_history_data(ticker_code):
    try:
        # [Fix] ä½¿ç”¨ requests å½è£ Session + yfinance
        session = requests.Session()
        session.headers.update({"User-Agent": "Mozilla/5.0"})
        
        # [Fix] å»¶é²
        time.sleep(1.5)
        
        # [Fix] ä¸ä½¿ç”¨ threadsï¼Œé™ä½ä½µç™¼è¢«æ“‹æ©Ÿç‡
        df = yf.Ticker(ticker_code, session=session).history(period="1y", auto_adjust=False)
        
        if df.empty: return pd.DataFrame()
        # [Fix] å®‰å…¨ç§»é™¤æ™‚å€
        if df.index.tz is not None: df.index = df.index.tz_localize(None)
        return df
    except: return pd.DataFrame()

def load_precise_db_from_sheet(sh):
    try:
        ws = sh.worksheet(PARAM_SHEET_NAME)
        data = ws.get_all_records()
        db = {}
        for row in data:
            code = str(row.get('ä»£è™Ÿ', '')).strip()
            if not code: continue
            try: shares = int(str(row.get('ç™¼è¡Œè‚¡æ•¸', 1)).replace(',', ''))
            except: shares = 1
            db[code] = {"market": str(row.get('å¸‚å ´', 'ä¸Šå¸‚')).strip(), "shares": shares}
        return db
    except: return {}

def fetch_stock_fundamental(stock_id, ticker_code, precise_db):
    market = 'ä¸Šå¸‚'; shares = 0
    if str(stock_id) in precise_db:
        db = precise_db[str(stock_id)]
        market = db['market']; shares = db['shares']
    data = {'shares': shares, 'market_type': market, 'pe': -1, 'pb': -1}
    try:
        time.sleep(1) # å»¶é²
        t = yf.Ticker(ticker_code)
        if ".TWO" in ticker_code: data['market_type'] = 'ä¸Šæ«ƒ'
        if data['shares'] <= 1:
            s = t.fast_info.get('shares', None)
            if s: data['shares'] = int(s)
        data['pe'] = t.info.get('trailingPE', t.info.get('forwardPE', 0))
        data['pb'] = t.info.get('priceToBook', 0)
        if data['pe']: data['pe'] = round(data['pe'], 2)
        if data['pb']: data['pb'] = round(data['pb'], 2)
    except: pass
    return data

# ============================
# ğŸ”¥ [æ ¸å¿ƒé‡å¯«] å®˜æ–¹æ¢æ–‡åš´æ ¼åˆ¤å®š
# ============================
def calc_pct(curr, ref):
    return ((curr - ref) / ref) * 100 if ref != 0 else 0

def calculate_full_risk(stock_id, hist_df, fund_data, est_days, dt_today_pct, dt_avg6_pct):
    res = {'risk_level': 'ä½', 'trigger_msg': '', 'curr_price': 0, 'limit_price': 0, 'gap_pct': 999.0, 'curr_vol': 0, 'limit_vol': 0, 'turnover_val': 0, 'turnover_rate': 0, 'pe': fund_data.get('pe', 0), 'pb': fund_data.get('pb', 0), 'day_trade_pct': dt_today_pct, 'is_triggered': False}

    if hist_df.empty or len(hist_df) < 7:
        if est_days <= 1: res['risk_level'] = 'é«˜'
        elif est_days <= 2: res['risk_level'] = 'ä¸­'
        return res

    curr_close = float(hist_df.iloc[-1]['Close'])
    curr_vol_shares = float(hist_df.iloc[-1]['Volume'])
    curr_vol_lots = int(curr_vol_shares / UNIT_LOT)

    shares = fund_data.get('shares', 1)
    if shares > 1: turnover = (curr_vol_shares / shares) * 100
    else: turnover = -1.0

    turnover_val_money = curr_close * curr_vol_shares

    res['curr_price'] = round(curr_close, 2)
    res['curr_vol'] = curr_vol_lots
    res['turnover_rate'] = round(turnover, 2)
    res['turnover_val'] = round(turnover_val_money / 100000000, 2)

    if curr_close < 5: return res

    triggers = []
    window_7 = hist_df.tail(7)
    ref_6 = float(window_7.iloc[0]['Close'])
    rise_6 = calc_pct(curr_close, ref_6)
    price_diff_6 = abs(curr_close - ref_6)

    cond_1 = rise_6 > 32
    cond_2 = (rise_6 > 25) and (price_diff_6 >= 50)

    if cond_1: triggers.append(f"ã€ç¬¬ä¸€æ¬¾ã€‘6æ—¥æ¼²{rise_6:.1f}%(>32%)")
    elif cond_2: triggers.append(f"ã€ç¬¬ä¸€æ¬¾ã€‘6æ—¥æ¼²{rise_6:.1f}%ä¸”åƒ¹å·®{price_diff_6:.0f}å…ƒ")

    limit_p1 = ref_6 * 1.32
    limit_p2 = ref_6 * 1.25 if price_diff_6 >= 50 else 99999
    final_limit = min(limit_p1, limit_p2) if cond_2 else limit_p1
    res['limit_price'] = round(final_limit, 2)
    res['gap_pct'] = round(((final_limit - curr_close)/curr_close)*100, 1)

    if len(hist_df) >= 31:
        w = hist_df.tail(31)
        rise_30 = calc_pct(curr_close, float(w.iloc[0]['Close']))
        if rise_30 > 100: triggers.append(f"ã€ç¬¬äºŒæ¬¾ã€‘30æ—¥æ¼²{rise_30:.0f}%")
    if len(hist_df) >= 61:
        w = hist_df.tail(61)
        rise_60 = calc_pct(curr_close, float(w.iloc[0]['Close']))
        if rise_60 > 130: triggers.append(f"ã€ç¬¬äºŒæ¬¾ã€‘60æ—¥æ¼²{rise_60:.0f}%")
    if len(hist_df) >= 91:
        w = hist_df.tail(91)
        rise_90 = calc_pct(curr_close, float(w.iloc[0]['Close']))
        if rise_90 > 160: triggers.append(f"ã€ç¬¬äºŒæ¬¾ã€‘90æ—¥æ¼²{rise_90:.0f}%")

    if len(hist_df) >= 61:
        avg_vol_60 = hist_df['Volume'].iloc[-61:-1].mean()
        if avg_vol_60 > 0:
            vol_ratio = curr_vol_shares / avg_vol_60
            res['limit_vol'] = int(avg_vol_60 * 5 / 1000)
            if turnover >= 0.1 and curr_vol_lots >= 500:
                if rise_6 > 25 and vol_ratio > 5:
                    triggers.append(f"ã€ç¬¬ä¸‰æ¬¾ã€‘æ¼²{rise_6:.0f}%+é‡{vol_ratio:.1f}å€")

    if turnover > 10 and rise_6 > 25:
        triggers.append(f"ã€ç¬¬å››æ¬¾ã€‘æ¼²{rise_6:.0f}%+è½‰{turnover:.0f}%")

    if len(hist_df) >= 61:
        avg_vol_60 = hist_df['Volume'].iloc[-61:-1].mean()
        avg_vol_6 = hist_df['Volume'].iloc[-6:].mean()
        is_exclude = (turnover < 0.1) or (curr_vol_lots < 500) or (turnover_val_money < 30000000)
        if not is_exclude and avg_vol_60 > 0:
            r1 = avg_vol_6 / avg_vol_60
            r2 = curr_vol_shares / avg_vol_60
            if r1 > 5: triggers.append(f"ã€ç¬¬ä¹æ¬¾ã€‘6æ—¥å‡é‡æ”¾å¤§{r1:.1f}å€")
            if r2 > 5: triggers.append(f"ã€ç¬¬ä¹æ¬¾ã€‘ç•¶æ—¥é‡æ”¾å¤§{r2:.1f}å€")

    if turnover > 0:
        acc_vol_6 = hist_df['Volume'].iloc[-6:].sum()
        acc_turn = (acc_vol_6 / shares) * 100
        if turnover_val_money >= 500000000:
            if acc_turn > 50 and turnover > 10:
                triggers.append(f"ã€ç¬¬åæ¬¾ã€‘ç´¯è½‰{acc_turn:.0f}%")

    if len(hist_df) >= 6:
        window_6 = hist_df.tail(6)
        high_6 = window_6['High'].max()
        low_6 = window_6['Low'].min()
        gap = high_6 - low_6
        threshold = 100
        if curr_close >= 500:
            tiers = int((curr_close - 500) / 500) + 1
            threshold = 100 + (tiers * 25)
        if gap >= threshold:
            triggers.append(f"ã€ç¬¬åä¸€æ¬¾ã€‘6æ—¥åƒ¹å·®{gap:.0f}å…ƒ(>é–€æª»{threshold})")

    if dt_avg6_pct > 60 and dt_today_pct > 60:
        dt_vol_est = curr_vol_shares * (dt_today_pct / 100.0)
        dt_vol_lots = dt_vol_est / 1000
        is_exclude = (turnover < 5) or (turnover_val_money < 500000000) or (dt_vol_lots < 5000)
        if not is_exclude:
            triggers.append(f"ã€ç¬¬åä¸‰æ¬¾ã€‘ç•¶æ²–{dt_today_pct}%(6æ—¥{dt_avg6_pct}%)")

    if triggers:
        res['is_triggered'] = True
        res['risk_level'] = 'é«˜'
        res['trigger_msg'] = "ä¸”".join(triggers)
    elif est_days <= 1: res['risk_level'] = 'é«˜'
    elif est_days <= 2: res['risk_level'] = 'ä¸­'
    elif est_days >= 3: res['risk_level'] = 'ä½'

    return res

# ============================
# ğŸ”¥ [æ–°å¢] ç¾æ³è™•ç½®æª¢æŸ¥
# ============================
def check_jail_trigger_now(status_list, clause_list):
    status_list = list(status_list)
    clause_list = list(clause_list)

    if len(status_list) < 30:
        pad = 30 - len(status_list)
        status_list = [0]*pad + status_list
        clause_list = [""]*pad + clause_list

    c1_streak = 0
    for c in clause_list[-3:]:
        if 1 in parse_clause_ids_strict(c): c1_streak += 1

    valid_cnt_5 = 0; valid_cnt_10 = 0; valid_cnt_30 = 0
    total_len = len(status_list)
    for i in range(30):
        idx = total_len - 1 - i
        if idx < 0: break
        if status_list[idx] == 1:
            ids = parse_clause_ids_strict(clause_list[idx])
            if is_valid_accumulation_day(ids):
                if i < 5: valid_cnt_5 += 1
                if i < 10: valid_cnt_10 += 1
                valid_cnt_30 += 1

    reasons = []
    if c1_streak == 3: reasons.append("å·²è§¸ç™¼(é€£3ç¬¬ä¸€æ¬¾)")
    if valid_cnt_5 == 5: reasons.append("å·²è§¸ç™¼(é€£5)")
    if valid_cnt_10 >= 6: reasons.append(f"å·²è§¸ç™¼(10æ—¥{valid_cnt_10}æ¬¡)")
    if valid_cnt_30 >= 12: reasons.append(f"å·²è§¸ç™¼(30æ—¥{valid_cnt_30}æ¬¡)")

    return (len(reasons) > 0), " | ".join(reasons)

# ============================
# ğŸ”¥ è™•ç½®é æ¸¬ (Fix: é¡¯ç¤ºå„ªåŒ– & ç‹€æ…‹åˆ¤æ–·)
# ============================
def simulate_days_to_jail_strict(status_list, clause_list, *, stock_id=None, target_date=None, jail_map=None, enable_safe_filter=True):
    if stock_id and target_date and jail_map and is_in_jail(stock_id, target_date, jail_map):
        return 0, "è™•ç½®ä¸­"

    trigger_now, reason_now = check_jail_trigger_now(status_list, clause_list)
    if trigger_now:
        return 0, reason_now.replace("å·²è§¸ç™¼", "å·²é”æ¨™ï¼Œæ¬¡ä¸€ç‡Ÿæ¥­æ—¥è™•ç½®")

    if enable_safe_filter:
        recent_valid_10 = 0
        check_len = min(len(status_list), 10)
        if check_len > 0:
            for b, c in zip(status_list[-check_len:], clause_list[-check_len:]):
                if b == 1:
                    ids = parse_clause_ids_strict(c)
                    if is_valid_accumulation_day(ids): recent_valid_10 += 1
        if recent_valid_10 == 0: return 99, "X"

    status_list = list(status_list)
    clause_list = list(clause_list)

    if len(status_list) < 30:
        pad = 30 - len(status_list)
        status_list = [0]*pad + status_list
        clause_list = [""]*pad + clause_list

    days = 0
    while days < 10:
        days += 1
        status_list.append(1)
        clause_list.append("ç¬¬1æ¬¾")

        c1_streak = 0
        for c in clause_list[-3:]:
            if 1 in parse_clause_ids_strict(c): c1_streak += 1

        valid_cnt_5 = 0; valid_cnt_10 = 0; valid_cnt_30 = 0
        total_len = len(status_list)
        for i in range(30):
            idx = total_len - 1 - i
            if idx < 0: break
            if status_list[idx] == 1:
                ids = parse_clause_ids_strict(clause_list[idx])
                if is_valid_accumulation_day(ids):
                    if i < 5: valid_cnt_5 += 1
                    if i < 10: valid_cnt_10 += 1
                    valid_cnt_30 += 1

        reasons = []
        if c1_streak == 3: reasons.append(f"å†{days}å¤©è™•ç½®")
        if valid_cnt_5 == 5: reasons.append(f"å†{days}å¤©è™•ç½®(é€£5)")
        if valid_cnt_10 >= 6: reasons.append(f"å†{days}å¤©è™•ç½®(10æ—¥{valid_cnt_10}æ¬¡)")
        if valid_cnt_30 >= 12: reasons.append(f"å†{days}å¤©è™•ç½®(30æ—¥{valid_cnt_30}æ¬¡)")

        if reasons: return days, " | ".join(reasons)

    return 99, ""

# ============================
# ğŸ”¥ Zeabur å°ˆç”¨é€£ç·š (è‡ªå‹•åˆ‡æ›)
# ============================
def connect_google_sheets():
    print("æ­£åœ¨é€²è¡Œ Google é©—è­‰...")
    try:
        key_path = "/service_key.json"
        if not os.path.exists(key_path):
            key_path = "service_key.json"
            
        if os.path.exists(key_path):
            gc = gspread.service_account(filename=key_path)
        else:
            auth.authenticate_user()
            creds, _ = default()
            gc = gspread.authorize(creds)
            
        try: sh = gc.open(SHEET_NAME)
        except: sh = gc.create(SHEET_NAME)
        return sh, None
    except Exception as e:
        print(f"âŒ Google Sheet é€£ç·šå¤±æ•—: {e}")
        return None, None

def main():
    sh, _ = connect_google_sheets()
    if not sh: return

    update_market_monitoring_log(sh)

    cal_dates = get_official_trading_calendar(240)
    target_trade_date_obj = cal_dates[-1]

    official_stocks = get_daily_data(target_trade_date_obj)

    is_today = (target_trade_date_obj == TARGET_DATE.date())
    is_early = (TARGET_DATE.time() < SAFE_CRAWL_TIME)
    is_pending = (official_stocks == [] and is_today and is_early)

    if official_stocks is None or is_pending:
        if len(cal_dates) >= 2:
            print("ğŸ”„ å•Ÿå‹•ã€Œæ™‚å…‰å›æœ”æ©Ÿåˆ¶ã€ï¼Œé€€å›ä¸Šä¸€å€‹äº¤æ˜“æ—¥ (T-1)...")
            cal_dates = cal_dates[:-1]
            target_trade_date_obj = cal_dates[-1]
            official_stocks = get_daily_data(target_trade_date_obj)
        else:
            print("âŒ äº¤æ˜“æ—¥æ›†ä¸è¶³ï¼Œç„¡æ³•å›æœ”ï¼Œç¶­æŒåŸæ—¥æœŸã€‚")

    target_date_str = target_trade_date_obj.strftime("%Y-%m-%d")
    print(f"ğŸ“… æœ€çµ‚é–å®šé‹ç®—æ—¥æœŸ: {target_date_str}")

    ws_log = get_or_create_ws(sh, "æ¯æ—¥ç´€éŒ„", headers=['æ—¥æœŸ','å¸‚å ´','ä»£è™Ÿ','åç¨±','è§¸çŠ¯æ¢æ¬¾'])

    total_log_rows = 0
    try:
        col1 = ws_log.col_values(1)
        total_log_rows = len(col1)
    except: pass

    if official_stocks:
        print(f"ğŸ’¾ å¯«å…¥è³‡æ–™åº«...")
        existing_keys = set()

        def strict_date_str(raw):
            try: return pd.to_datetime(str(raw).strip()).strftime("%Y-%m-%d")
            except: return str(raw).strip()

        if total_log_rows < 2:
            try:
                existing_data = ws_log.get_all_values()
                if len(existing_data) > 1:
                    for row in existing_data[1:]:
                        if len(row) >= 3 and row[0] != 'æ—¥æœŸ' and str(row[2]).isdigit():
                            d_std = strict_date_str(row[0])
                            existing_keys.add(f"{d_std}_{row[2]}")
                total_log_rows = len(existing_data)
            except: pass
        else:
            try:
                start_row = max(1, total_log_rows - 3000)
                raw_keys = ws_log.get(f'A{start_row}:E{total_log_rows}')
                if raw_keys:
                    for r in raw_keys:
                        if len(r) >= 3 and r[0] != 'æ—¥æœŸ' and str(r[2]).isdigit():
                            d_std = strict_date_str(r[0])
                            existing_keys.add(f"{d_std}_{r[2]}")
            except: pass

        new_rows = []
        today_codes = set([s['ä»£è™Ÿ'] for s in official_stocks])

        for stock in official_stocks:
            if stock['ä»£è™Ÿ'] not in today_codes: continue
            key = f"{stock['æ—¥æœŸ']}_{stock['ä»£è™Ÿ']}"
            if key not in existing_keys:
                new_rows.append([stock['æ—¥æœŸ'], stock['å¸‚å ´'], stock['ä»£è™Ÿ'], stock['åç¨±'], stock['è§¸çŠ¯æ¢æ¬¾']])

        if new_rows:
            ws_log.append_rows(new_rows, value_input_option='USER_ENTERED')
            total_log_rows += len(new_rows)

    precise_db_cache = load_precise_db_from_sheet(sh)
    print("ğŸ“Š æ­£åœ¨åŒæ­¥å¤§ç›¤è³‡æ–™...")
    finmind_trade_date_str = target_date_str

    try:
        if total_log_rows < 2: raise ValueError("Too small")
        limit = 8000
        start_idx = max(1, total_log_rows - limit)
        raw_vals = ws_log.get(f'A{start_idx}:E{total_log_rows}')
        if not raw_vals or len(raw_vals) < 2: raise ValueError("Empty range")
        if start_idx > 1:
            headers = ws_log.get('A1:E1')
            if headers: raw_vals = headers + raw_vals

        df = pd.DataFrame(raw_vals[1:], columns=raw_vals[0])
        df.columns = [str(c).strip() for c in df.columns]
        req_cols = {'æ—¥æœŸ', 'ä»£è™Ÿ', 'åç¨±', 'è§¸çŠ¯æ¢æ¬¾'}
        if not req_cols.issubset(set(df.columns)): raise ValueError(f"Missing columns: {req_cols - set(df.columns)}")

        tmp_dates = pd.to_datetime(df['æ—¥æœŸ'], errors='coerce')
        min_ts = tmp_dates.dropna().min()
        start_date_90 = cal_dates[-90] if len(cal_dates) >= 90 else cal_dates[0]
        start_ts = pd.Timestamp(start_date_90)

        if pd.notna(min_ts) and min_ts > start_ts:
            print("âš ï¸ ç·©è¡å€é–“ä¸è¶³ï¼Œæ”¹ç‚ºè®€å–å…¨è¡¨...")
            all_vals = ws_log.get_all_values()
            if not all_vals or len(all_vals) < 2:
                df = pd.DataFrame(columns=['æ—¥æœŸ','å¸‚å ´','ä»£è™Ÿ','åç¨±','è§¸çŠ¯æ¢æ¬¾'])
            else:
                df = pd.DataFrame(all_vals[1:], columns=all_vals[0])
                df.columns = [str(c).strip() for c in df.columns]
                df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'], errors='coerce').dt.date
        else:
            df['æ—¥æœŸ'] = tmp_dates.dt.date

    except Exception as e:
        print(f"âš ï¸ è®€å–å„ªåŒ–å¤±æ•— ({e})ï¼Œé™ç´šç‚ºå…¨è¡¨è®€å–...")
        all_vals = ws_log.get_all_values()
        if not all_vals or len(all_vals) < 2:
            df = pd.DataFrame(columns=['æ—¥æœŸ','å¸‚å ´','ä»£è™Ÿ','åç¨±','è§¸çŠ¯æ¢æ¬¾'])
        else:
            df = pd.DataFrame(all_vals[1:], columns=all_vals[0])
            df.columns = [str(c).strip() for c in df.columns]
            df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'], errors='coerce').dt.date

    if not df.empty:
        df['ä»£è™Ÿ'] = df['ä»£è™Ÿ'].astype(str).str.strip()
        df = df[df['ä»£è™Ÿ'].str.match(r'^\d{4}$', na=False)]
        df = df[pd.notna(df['æ—¥æœŸ'])]

    clause_map = {}
    for _, r in df.iterrows():
        try: 
            k = (str(r['ä»£è™Ÿ']), r['æ—¥æœŸ'])
            new_text = str(r.get('è§¸çŠ¯æ¢æ¬¾', '') or '')
            old_text = clause_map.get(k, "")
            clause_map[k] = merge_clause_text(old_text, new_text)
        except: pass

    df_recent = df[df['æ—¥æœŸ'] >= start_date_90]
    stock_list = df_recent['ä»£è™Ÿ'].unique()
    target_stocks = stock_list
    total_scan = len(target_stocks)
    
    jail_lookback = target_trade_date_obj - timedelta(days=90)
    jail_map = get_jail_map(jail_lookback, target_trade_date_obj)
    exclude_map = build_exclude_map(cal_dates, jail_map)

    print(f"ğŸ” é–‹å§‹æƒæ {total_scan} æª”è‚¡ç¥¨ (V116.18 å®Œæ•´é‚è¼¯ç‰ˆ - å–®æ¬¡å…¨åŸ·è¡Œ)...")

    rows_stats = []

    for idx, code in enumerate(target_stocks):
        code = str(code).strip()
        name_series = df[df['ä»£è™Ÿ']==code]['åç¨±']
        name = name_series.iloc[-1] if not name_series.empty else "æœªçŸ¥"

        db_info = precise_db_cache.get(code, {})
        suffix = get_ticker_suffix(db_info.get('market', 'ä¸Šå¸‚'))
        ticker_code = f"{code}{suffix}"

        stock_calendar_30_asc = get_last_n_non_jail_trade_dates(code, cal_dates, jail_map, exclude_map=exclude_map, n=30)

        bits = []; clauses = []
        for d in stock_calendar_30_asc:
            c_str = clause_map.get((code, d), "")
            if is_excluded(code, d, exclude_map):
                bits.append(0); clauses.append(c_str)
            elif c_str:
                bits.append(1); clauses.append(c_str)
            else:
                bits.append(0); clauses.append("")

        valid_bits = []
        for i in range(len(bits)):
            if bits[i] == 1:
                ids = parse_clause_ids_strict(clauses[i])
                valid_bits.append(1 if is_valid_accumulation_day(ids) else 0)
            else: valid_bits.append(0)

        status_30 = "".join(map(str, valid_bits)).zfill(30)

        est_days, reason_msg = simulate_days_to_jail_strict(
            bits, clauses, stock_id=code, target_date=target_trade_date_obj,
            jail_map=jail_map, enable_safe_filter=False 
        )

        latest_ids = parse_clause_ids_strict(clauses[-1] if clauses else "")
        is_special_risk = is_special_risk_day(latest_ids)
        is_clause_13 = False
        for c in clauses:
            if 13 in parse_clause_ids_strict(c):
                is_clause_13 = True; break

        if reason_msg == "X":
            est_days_int = 99; est_days_display = "X"
            if is_special_risk:
                reason_display = "ç±Œç¢¼ç•°å¸¸(äººå·¥å¯©æ ¸é¢¨éšª)"
                if is_clause_13: reason_display += " + åˆ‘æœŸå¯èƒ½å»¶é•·"
            else: reason_display = ""
        elif est_days == 0:
             est_days_int = 0; est_days_display = "0"
             reason_display = reason_msg
        else:
            est_days_int = int(est_days); est_days_display = str(est_days_int)
            reason_display = reason_msg
            if is_special_risk: reason_display += " | âš ï¸ç•™æ„äººå·¥è™•ç½®é¢¨éšª"
            if is_clause_13: reason_display += " (è‹¥é€²è™•ç½®å°‡é—œ12å¤©)"

        hist = fetch_history_data(ticker_code)
        if hist.empty:
            alt_suffix = '.TWO' if suffix == '.TW' else '.TW'
            alt_ticker = f"{code}{alt_suffix}"
            hist = fetch_history_data(alt_ticker)
            if not hist.empty: ticker_code = alt_ticker

        fund = fetch_stock_fundamental(code, ticker_code, precise_db_cache)

        if (idx + 1) % 10 == 0: time.sleep(1.5)

        dt_today, dt_avg6 = get_daytrade_stats_finmind(code, finmind_trade_date_str)
        
        risk_res = calculate_full_risk(code, hist, fund, est_days_int, dt_today, dt_avg6)

        print(f"   [{idx+1}/{total_scan}] {code} {name}: æœ€å¿«{est_days_display}å¤© {reason_display} | {risk_res['trigger_msg']} | ç•¶æ²–:{dt_today}%")

        streak = 0
        for b in valid_bits[::-1]:
            if b == 1: streak += 1
            else: break

        last_trigger_date_str = "ç„¡"
        if len(valid_bits) > 0:
            for i in range(len(valid_bits)-1, -1, -1):
                if valid_bits[i] == 1:
                    last_trigger_date_str = stock_calendar_30_asc[i].strftime("%Y-%m-%d")
                    break
        
        cnt_30 = sum(valid_bits); cnt_10 = sum(valid_bits[-10:])

        rows_stats.append([
            code, name, streak, cnt_30, cnt_10, last_trigger_date_str,
            status_30, status_30[-10:], est_days_display, reason_display, risk_res['risk_level'], risk_res['trigger_msg'],
            risk_res['curr_price'], risk_res['limit_price'], risk_res['gap_pct'],
            risk_res['curr_vol'], risk_res['limit_vol'], risk_res['turnover_val'],
            risk_res['turnover_rate'], risk_res['pe'], risk_res['pb'],
            risk_res['day_trade_pct']
        ])

    try:
        ws_stats = get_or_create_ws(sh, "è¿‘30æ—¥ç†±é–€çµ±è¨ˆ", headers=STATS_HEADERS)
        print("ğŸ’¾ æ›´æ–° [è¿‘30æ—¥ç†±é–€çµ±è¨ˆ] (æ¸…ç©ºé‡å¯«)...")
        ws_stats.clear()
        ws_stats.append_row(STATS_HEADERS, value_input_option='USER_ENTERED')
        if rows_stats:
            ws_stats.append_rows(rows_stats, value_input_option='USER_ENTERED')

        print("\nâœ… V116.18 åŸ·è¡Œå®Œæˆï¼")
    except Exception as e:
        print(f"âŒ å¯«å…¥å¤±æ•—: {e}")

if __name__ == "__main__":
    main()
