# -*- coding: utf-8 -*-
"""
V116.19 å°è‚¡æ³¨æ„è‚¡ç³»çµ± (GitHub Action å–®æª”ç›´ä¸Šç‰ˆ - Risk Logic & Stability Final)
ä¿®æ­£é‡é»ï¼š
1. [é¢¨éšª] calculate_full_risk: ä¿®æ­£æ¬Šé‡é‚è¼¯ï¼Œå‰©é¤˜å¤©æ•¸ <= 2 ä¸€å¾‹å¼·åˆ¶åˆ¤å®šç‚ºã€Œé«˜é¢¨éšª(ç´…ç‡ˆ)ã€ï¼Œç¢ºä¿æ’åºå„ªå…ˆã€‚
2. [åŒæ­¥] åŒ…å«æ‰€æœ‰ 1223 ç‰ˆé‚è¼¯ (TWSEå‹•æ…‹æ¬„ä½ã€TPExå››ç¢¼/Warm-up/Retryã€å‰›å‡ºé—œæ­¸é›¶)ã€‚
3. [é˜²å‘†] ä¿æŒçˆ¬èŸ²å¤±æ•—å›å‚³ None æ©Ÿåˆ¶ï¼Œç¢ºä¿è³‡æ–™å®Œæ•´æ€§ã€‚
"""

import os
import twstock
import yfinance as yf
import pandas as pd
import numpy as np
import requests
import re
import time
import random
import gspread
import logging
from google.oauth2.service_account import Credentials
from datetime import datetime, timedelta, time as dt_time, date
from dateutil.relativedelta import relativedelta
from zoneinfo import ZoneInfo

# ==========================================
# 1. è¨­å®šéœéŸ³æ¨¡å¼èˆ‡å¸¸æ•¸
# ==========================================
logger = logging.getLogger('yfinance')
logger.setLevel(logging.CRITICAL)
logger.disabled = True

UNIT_LOT = 1000

# å®šç¾©çµ±è¨ˆè¡¨é ­
STATS_HEADERS = [
    'ä»£è™Ÿ', 'åç¨±', 'é€£çºŒå¤©æ•¸', 'è¿‘30æ—¥æ³¨æ„æ¬¡æ•¸', 'è¿‘10æ—¥æ³¨æ„æ¬¡æ•¸', 'æœ€è¿‘ä¸€æ¬¡æ—¥æœŸ',
    '30æ—¥ç‹€æ…‹ç¢¼', '10æ—¥ç‹€æ…‹ç¢¼', 'æœ€å¿«è™•ç½®å¤©æ•¸', 'è™•ç½®è§¸ç™¼åŸå› ', 'é¢¨éšªç­‰ç´š', 'è§¸ç™¼æ¢ä»¶',
    'ç›®å‰åƒ¹', 'è­¦æˆ’åƒ¹', 'å·®å¹…(%)', 'ç›®å‰é‡', 'è­¦æˆ’é‡', 'æˆäº¤å€¼(å„„)',
    'é€±è½‰ç‡(%)', 'PE', 'PB', 'ç•¶æ²–ä½”æ¯”(%)'
]

# ==========================================
# ğŸ“† è¨­å®šå€
# ==========================================
SHEET_NAME = "å°è‚¡æ³¨æ„è‚¡è³‡æ–™åº«_V33"
PARAM_SHEET_NAME = "å€‹è‚¡åƒæ•¸"
TW_TZ = ZoneInfo("Asia/Taipei")
TARGET_DATE = datetime.now(TW_TZ)

# æ™‚é–“é–€æª»
SAFE_CRAWL_TIME = dt_time(17, 30)        # å…¶ä»–è³‡è¨Šï¼ˆæ³¨æ„è‚¡/çµ±è¨ˆï¼‰å›ºå®š 17:30 å¾Œè·‘
DAYTRADE_PUBLISH_TIME = dt_time(21, 0)   # ç•¶æ²–ç‡ 21:00 å¾Œæ‰æŠ“
SAFE_MARKET_OPEN_CHECK = dt_time(16, 30) # ç”¨æ–¼åˆ¤æ–·æ—¥æ›†æ˜¯å¦è©²æœ‰ä»Šå¤©

IS_NIGHT_RUN = TARGET_DATE.hour >= 20
IS_AFTER_SAFE = TARGET_DATE.time() >= SAFE_CRAWL_TIME
IS_AFTER_DAYTRADE = TARGET_DATE.time() >= DAYTRADE_PUBLISH_TIME

# å›è£œåƒæ•¸
MAX_BACKFILL_TRADING_DAYS = 40   # æœ€å¤šå›è£œå¹¾å€‹äº¤æ˜“æ—¥(å¾€å‰)
VERIFY_RECENT_DAYS = 2           # å¼·åˆ¶é©—è­‰æœ€è¿‘å¹¾å€‹äº¤æ˜“æ—¥

# ==========================================
# ğŸ”‘ FinMind é‡‘é‘°è¨­å®š (GitHub Secret é©é…)
# ==========================================
FINMIND_API_URL = "https://api.finmindtrade.com/api/v4/data"

token1 = os.getenv('FinMind_1')
token2 = os.getenv('FinMind_2')
FINMIND_TOKENS = [t for t in [token1, token2] if t]

CURRENT_TOKEN_INDEX = 0
_FINMIND_CACHE = {}

print(f"ğŸš€ å•Ÿå‹• V116.19 å°è‚¡æ³¨æ„è‚¡ç³»çµ± (Final)")
print(f"ğŸ•’ ç³»çµ±æ™‚é–“ (Taiwan): {TARGET_DATE.strftime('%Y-%m-%d %H:%M:%S')}")
print(f"â° æ™‚åºç‹€æ…‹: After 17:30? {IS_AFTER_SAFE} | After 21:00? {IS_AFTER_DAYTRADE}")

try: twstock.__update_codes()
except: pass

# ============================
# ğŸ› ï¸ å·¥å…·å‡½å¼
# ============================
CN_NUM = {"ä¸€":"1","äºŒ":"2","ä¸‰":"3","å››":"4","äº”":"5","å…­":"6","ä¸ƒ":"7","å…«":"8","ä¹":"9","å":"10"}

KEYWORD_MAP = {
    "èµ·è¿„å…©å€‹ç‡Ÿæ¥­æ—¥": 11, "ç•¶æ—¥æ²–éŠ·": 13, "å€Ÿåˆ¸è³£å‡º": 12, "ç´¯ç©é€±è½‰ç‡": 10, "é€±è½‰ç‡": 4,
    "æˆäº¤é‡": 9, "æœ¬ç›Šæ¯”": 6, "è‚¡åƒ¹æ·¨å€¼æ¯”": 6, "æº¢æŠ˜åƒ¹": 8, "æ”¶ç›¤åƒ¹æ¼²è·Œç™¾åˆ†æ¯”": 1,
    "æœ€å¾Œæˆäº¤åƒ¹æ¼²è·Œ": 1, "æœ€è¿‘å…­å€‹ç‡Ÿæ¥­æ—¥ç´¯ç©": 1
}

def normalize_clause_text(s: str) -> str:
    if not s: return ""
    s = str(s)
    s = s.replace("ç¬¬ã„§æ¬¾", "ç¬¬ä¸€æ¬¾")
    for cn, dg in CN_NUM.items():
        s = s.replace(f"ç¬¬{cn}æ¬¾", f"ç¬¬{dg}æ¬¾")
    s = s.translate(str.maketrans("ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼", "1234567890"))
    return s

def parse_clause_ids_strict(clause_text):
    if not isinstance(clause_text, str): return set()
    clause_text = normalize_clause_text(clause_text)
    ids = set()
    matches = re.findall(r'ç¬¬\s*(\d+)\s*æ¬¾', clause_text)
    for m in matches: ids.add(int(m))
    if not ids:
        for keyword, code in KEYWORD_MAP.items():
            if keyword in clause_text: ids.add(code)
    return ids

def merge_clause_text(a, b):
    ids = set()
    ids |= parse_clause_ids_strict(a) if a else set()
    ids |= parse_clause_ids_strict(b) if b else set()
    if ids: return "ã€".join([f"ç¬¬{x}æ¬¾" for x in sorted(ids)])
    a = a or ""; b = b or ""
    return a if len(a) >= len(b) else b

def is_valid_accumulation_day(ids):
    if not ids: return False
    return any(1 <= x <= 8 for x in ids)

def is_special_risk_day(ids):
    if not ids: return False
    return any(9 <= x <= 14 for x in ids)

def get_ticker_suffix(market_type):
    m = str(market_type).upper().strip()
    keywords = ['ä¸Šæ«ƒ', 'TWO', 'TPEX', 'OTC']
    if any(k in m for k in keywords): return '.TWO'
    return '.TW'

def connect_google_sheets():
    try:
        if not os.path.exists("service_key.json"): return None, None
        gc = gspread.service_account(filename="service_key.json")
        try: sh = gc.open(SHEET_NAME)
        except: sh = gc.create(SHEET_NAME)
        return sh, None
    except: return None, None

def get_or_create_ws(sh, title, headers=None, rows=5000, cols=20):
    need_cols = max(cols, len(headers) if headers else 0)
    try:
        ws = sh.worksheet(title)
        try:
            if headers and ws.col_count < need_cols:
                ws.resize(rows=ws.row_count, cols=need_cols)
        except: pass
        return ws
    except:
        print(f"âš ï¸ å·¥ä½œè¡¨ '{title}' ä¸å­˜åœ¨ï¼Œæ­£åœ¨å»ºç«‹...")
        ws = sh.add_worksheet(title=title, rows=str(rows), cols=str(need_cols))
        if headers:
            ws.append_row(headers, value_input_option="USER_ENTERED")
        return ws

def load_log_index(ws_log):
    existing_keys = set()
    date_counts = {}
    try:
        vals = ws_log.get_all_values()
        if not vals or len(vals) <= 1: return existing_keys, date_counts
        for r in vals[1:]:
            if len(r) >= 3 and str(r[0]).strip():
                d = str(r[0]).strip()
                code = str(r[2]).strip().replace("'", "")
                if code:
                    k = d + "_" + code
                    existing_keys.add(k)
                    date_counts[d] = date_counts.get(d, 0) + 1
    except: pass
    return existing_keys, date_counts

def load_status_index(ws_status):
    key_to_row = {}
    cnt_map = {}
    try:
        vals = ws_status.get_all_values()
        if not vals or len(vals) <= 1: return key_to_row, cnt_map
        for r_idx, row in enumerate(vals[1:], start=2):
            if len(row) >= 1 and str(row[0]).strip():
                d = str(row[0]).strip()
                key_to_row[d] = r_idx
                c = 0
                if len(row) >= 2:
                    try: c = int(str(row[1]).strip())
                    except: c = 0
                cnt_map[d] = c
    except: pass
    return key_to_row, cnt_map

def upsert_status(ws_status, key_to_row, date_str, count, now_str):
    row_data = [date_str, int(count), now_str]
    if date_str in key_to_row:
        r = key_to_row[date_str]
        try: ws_status.update(values=[row_data], range_name=f"A{r}:C{r}", value_input_option="USER_ENTERED")
        except: pass
    else:
        try: ws_status.append_row(row_data, value_input_option="USER_ENTERED")
        except: pass

def finmind_get(dataset, data_id=None, start_date=None, end_date=None):
    global CURRENT_TOKEN_INDEX
    cache_key = (dataset, data_id, start_date, end_date)
    if cache_key in _FINMIND_CACHE: return _FINMIND_CACHE[cache_key].copy()

    params = {"dataset": dataset}
    if data_id: params["data_id"] = str(data_id)
    if start_date: params["start_date"] = start_date
    if end_date: params["end_date"] = end_date
    if not FINMIND_TOKENS: return pd.DataFrame()

    for _ in range(4):
        headers = {"Authorization": f"Bearer {FINMIND_TOKENS[CURRENT_TOKEN_INDEX]}", "User-Agent": "Mozilla/5.0", "Connection": "close"}
        try:
            r = requests.get(FINMIND_API_URL, params=params, headers=headers, timeout=10)
            if r.status_code == 200:
                j = r.json()
                df = pd.DataFrame(j.get("data", [])) if "data" in j else pd.DataFrame()
                if len(_FINMIND_CACHE) >= 2000: _FINMIND_CACHE.clear()
                _FINMIND_CACHE[cache_key] = df
                return df.copy()
            elif r.status_code != 200:
                time.sleep(2)
                CURRENT_TOKEN_INDEX = (CURRENT_TOKEN_INDEX + 1) % len(FINMIND_TOKENS)
                continue
        except: time.sleep(1)
    return pd.DataFrame()

def update_market_monitoring_log(sh):
    print("ğŸ“Š æª¢æŸ¥ä¸¦æ›´æ–°ã€Œå¤§ç›¤æ•¸æ“šç›£æ§ã€...")
    HEADERS = ['æ—¥æœŸ', 'ä»£è™Ÿ', 'åç¨±', 'æ”¶ç›¤åƒ¹', 'æ¼²è·Œå¹…(%)', 'æˆäº¤é‡‘é¡(å„„)']
    ws_market = get_or_create_ws(sh, "å¤§ç›¤æ•¸æ“šç›£æ§", headers=HEADERS, cols=10)

    def norm_date(s):
        s = str(s).strip()
        if not s: return ""
        try: return pd.to_datetime(s, errors='coerce').strftime("%Y-%m-%d")
        except: return s

    key_to_row = {}
    try:
        all_vals = ws_market.get_all_values()
        for r_idx, row in enumerate(all_vals[1:], start=2):
            if len(row) >= 2:
                key_to_row[f"{norm_date(row[0])}_{str(row[1]).strip()}"] = r_idx
    except: pass

    existing_keys = set(key_to_row.keys())

    try:
        targets = [
            {'fin_id': 'TAIEX', 'code': '^TWII', 'name': 'åŠ æ¬ŠæŒ‡æ•¸'},
            {'fin_id': 'TPEx',  'code': '^TWOII', 'name': 'æ«ƒè²·æŒ‡æ•¸'}
        ]
        start_date_str = (TARGET_DATE - timedelta(days=45)).strftime("%Y-%m-%d")
        dfs = {}
        for t in targets:
            df = finmind_get("TaiwanStockPrice", data_id=t['fin_id'], start_date=start_date_str)
            if not df.empty:
                df['date'] = pd.to_datetime(df['date'])
                df.set_index('date', inplace=True)
                df.index = df.index.tz_localize(None)
                if 'close' in df.columns:
                    df['Close'] = df['close'].astype(float)
                    df['Pct'] = df['Close'].pct_change() * 100
                if 'Turnover' in df.columns: df['Volume'] = df['Turnover'].astype(float)
                elif 'Trading_money' in df.columns: df['Volume'] = df['Trading_money'].astype(float)
                else: df['Volume'] = 0.0
                dfs[t['code']] = df

        new_rows = []
        today_str = TARGET_DATE.strftime("%Y-%m-%d")
        all_dates = set()
        for df in dfs.values(): all_dates.update(df.index.strftime("%Y-%m-%d").tolist())

        for d in sorted(all_dates):
            for t in targets:
                code = t['code']; name = t['name']
                df = dfs.get(code)
                if df is None or d not in df.index.strftime("%Y-%m-%d"): continue
                try: row = df.loc[d]
                except: row = df[df.index.strftime("%Y-%m-%d") == d].iloc[0]
                
                if pd.isna(row.get('Close')): continue
                close = round(float(row['Close']), 2)
                pct = round(float(row.get('Pct', 0) or 0), 2)
                vol = round(float(row.get('Volume', 0) or 0) / 100000000, 2)
                
                row_data = [d, code, name, close, pct, vol]
                comp_key = f"{d}_{code}"

                if d == today_str and TARGET_DATE.time() < SAFE_MARKET_OPEN_CHECK: continue
                if d == today_str and comp_key in key_to_row and TARGET_DATE.time() >= SAFE_MARKET_OPEN_CHECK:
                    try:
                        r_num = key_to_row[comp_key]
                        ws_market.update(values=[row_data], range_name=f'A{r_num}:F{r_num}', value_input_option="USER_ENTERED")
                    except: pass
                    continue
                if comp_key in existing_keys: continue
                if close > 0: new_rows.append(row_data)

        if new_rows: ws_market.append_rows(new_rows, value_input_option="USER_ENTERED")
    except Exception as e: print(f" âŒ å¤§ç›¤æ›´æ–°å¤±æ•—: {e}")

# ============================
# ğŸ”¥ è™•ç½®è³‡æ–™æŠ“å– (Jail)
# ============================
def parse_roc_date(roc_date_str):
    try:
        roc_date_str = str(roc_date_str).strip()
        parts = re.split(r"[/-]", roc_date_str)
        if len(parts) == 3:
            y = int(parts[0]) + 1911
            m = int(parts[1])
            d = int(parts[2])
            return date(y, m, d)
    except:
        return None
    return None

def parse_jail_period(period_str):
    if not period_str:
        return None, None

    s = str(period_str).strip()
    dates = []
    if "ï½" in s:
        dates = s.split("ï½")
    elif "~" in s:
        dates = s.split("~")
    elif "-" in s and "/" in s and s.count("-") == 1:
        dates = s.split("-")

    if len(dates) >= 2:
        sd = parse_roc_date(dates[0].strip())
        ed = parse_roc_date(dates[1].strip())
        if sd and ed:
            return sd, ed
    return None, None

def get_jail_map(start_date_obj, end_date_obj):
    print("ğŸ”’ æ­£åœ¨ä¸‹è¼‰è™•ç½®(Jail)åå–®ä»¥å»ºç«‹æ¿¾ç¶²...")
    jail_map = {}

    s_str = start_date_obj.strftime("%Y%m%d")
    e_str = end_date_obj.strftime("%Y%m%d")

    # 1) TWSE (ä¸Šå¸‚) - å‹•æ…‹æ¬„ä½è§£æ
    try:
        url = "https://www.twse.com.tw/rwd/zh/announcement/punish"
        headers = {"User-Agent": "Mozilla/5.0"}

        r = requests.get(
            url,
            params={"startDate": s_str, "endDate": e_str, "response": "json"},
            headers=headers,
            timeout=10,
        )
        j = r.json()

        if isinstance(j.get("tables"), list) and j["tables"]:
            t = j["tables"][0]
            fields = t.get("fields", []) or []
            data_rows = t.get("data", []) or []

            def find_idx(keys):
                for i, f in enumerate(fields):
                    fs = str(f)
                    if any(k in fs for k in keys):
                        return i
                return None

            idx_code = find_idx(["è­‰åˆ¸ä»£è™Ÿ", "ä»£è™Ÿ", "æœ‰åƒ¹è­‰åˆ¸ä»£è™Ÿ"])
            idx_period = find_idx(["è™•ç½®èµ·è¿„æ™‚é–“", "è™•ç½®èµ·è¨–æ™‚é–“", "è™•ç½®æœŸé–“", "èµ·è¿„"])

            # fallback
            if idx_code is None: idx_code = 2
            if idx_period is None: idx_period = 6

            for row in data_rows:
                try:
                    code = str(row[idx_code]).strip()
                    p = str(row[idx_period]).strip()
                    sd, ed = parse_jail_period(p)
                    if sd and ed:
                        jail_map.setdefault(code, []).append((sd, ed))
                except:
                    continue
    except:
        pass

    # 2) TPEx (ä¸Šæ«ƒ) - å››ç¢¼æª¢æŸ¥
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        r = requests.get("https://www.tpex.org.tw/openapi/v1/tpex_disposal_information", headers=headers, timeout=10)
        for item in r.json():
            try:
                code = str(item.get("SecuritiesCompanyCode", "")).strip()
                if not (code.isdigit() and len(code) == 4):
                    continue

                sd, ed = parse_jail_period(str(item.get("DispositionPeriod", "")).strip())
                if sd and ed and ed >= start_date_obj and sd <= end_date_obj:
                    jail_map.setdefault(code, []).append((sd, ed))
            except:
                continue
    except:
        pass

    for k in jail_map:
        jail_map[k] = sorted(jail_map[k], key=lambda x: x[0])
    return jail_map

def is_in_jail(stock_id, target_date, jail_map):
    if not jail_map or stock_id not in jail_map:
        return False
    for s, e in jail_map[stock_id]:
        if s <= target_date <= e:
            return True
    return False

def prev_trade_date(d, cal_dates):
    try:
        idx = cal_dates.index(d)
        return cal_dates[idx - 1] if idx > 0 else None
    except:
        for i in range(len(cal_dates) - 1, -1, -1):
            if cal_dates[i] < d:
                return cal_dates[i]
        return None

def build_exclude_map(cal_dates, jail_map):
    exclude_map = {}
    if not jail_map:
        return exclude_map

    for code, periods in jail_map.items():
        s = set()
        for start, end in periods:
            pd = prev_trade_date(start, cal_dates)
            if pd:
                s.add(pd)
            for d in cal_dates:
                if start <= d <= end:
                    s.add(d)
        exclude_map[code] = s
    return exclude_map

def is_excluded(code, d, exclude_map):
    return bool(exclude_map) and (code in exclude_map) and (d in exclude_map[code])

def get_last_n_non_jail_trade_dates(stock_id, cal_dates, jail_map, exclude_map=None, n=30):
    last_jail_end = date(1900, 1, 1)
    if jail_map and stock_id in jail_map and jail_map[stock_id]:
        last_jail_end = jail_map[stock_id][-1][1]

    picked = []
    for d in reversed(cal_dates):
        if d <= last_jail_end:
            break
        if is_excluded(stock_id, d, exclude_map):
            continue
        if jail_map and is_in_jail(stock_id, d, jail_map):
            continue
        picked.append(d)
        if len(picked) >= n:
            break

    return list(reversed(picked))

# ============================
# ğŸ”¥ æ¯æ—¥å…¬å‘Šçˆ¬èŸ²å€
# ============================
def fetch_twse_attention_rows(date_obj, date_str):
    date_str_nodash = date_obj.strftime("%Y%m%d")
    rows = []
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        r = requests.get(
            "https://www.twse.com.tw/rwd/zh/announcement/notice",
            params={"startDate": date_str_nodash, "endDate": date_str_nodash, "response": "json"},
            headers=headers,
            timeout=10,
        )
        if r.status_code != 200:
            return None

        d = r.json()
        for i in d.get("data", []) or []:
            code = str(i[1]).strip()
            name = str(i[2]).strip()
            if len(code) == 4 and code.isdigit():
                raw = " ".join([str(x) for x in i])
                ids = parse_clause_ids_strict(raw)
                c_str = "ã€".join([f"ç¬¬{k}æ¬¾" for k in sorted(ids)]) or raw
                rows.append({"æ—¥æœŸ": date_str, "å¸‚å ´": "TWSE", "ä»£è™Ÿ": code, "åç¨±": name, "è§¸çŠ¯æ¢æ¬¾": c_str})
    except:
        return None
    return rows

def fetch_tpex_attention_rows(date_obj, date_str):
    roc_date = f"{date_obj.year - 1911}/{date_obj.month:02d}/{date_obj.day:02d}"
    url = "https://www.tpex.org.tw/www/zh-tw/bulletin/attention"

    headers = {
        "User-Agent": "Mozilla/5.0",
        "Referer": "https://www.tpex.org.tw/",
        "Origin": "https://www.tpex.org.tw",
        "Accept": "application/json, text/plain, */*",
        "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
        "X-Requested-With": "XMLHttpRequest",
    }
    payload = {"date": roc_date, "response": "json"}

    s = requests.Session()

    try:
        s.get("https://www.tpex.org.tw/", headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
    except:
        pass

    for attempt in range(1, 4):
        try:
            r = s.post(url, data=payload, headers=headers, timeout=12)
            if r.status_code != 200:
                time.sleep(0.8)
                continue

            res = r.json()

            target = []
            if "tables" in res:
                for t in res["tables"]:
                    target.extend(t.get("data", []) or [])
            else:
                target = res.get("data", []) or []

            rows = []
            for i in target:
                if len(i) <= 5:
                    continue
                row_date = str(i[5]).strip()
                if row_date not in (roc_date, date_str):
                    continue

                code = str(i[1]).strip()
                name = str(i[2]).strip()
                if not (code.isdigit() and len(code) == 4):
                    continue

                raw = " ".join([str(x) for x in i])
                ids = parse_clause_ids_strict(raw)
                c_str = "ã€".join([f"ç¬¬{k}æ¬¾" for k in sorted(ids)]) if ids else raw

                rows.append({"æ—¥æœŸ": date_str, "å¸‚å ´": "TPEx", "ä»£è™Ÿ": code, "åç¨±": name, "è§¸çŠ¯æ¢æ¬¾": c_str})

            return rows
        except:
            time.sleep(0.8)

    return None

def get_daily_data(date_obj):
    date_str = date_obj.strftime("%Y-%m-%d")
    print(f"ğŸ“¡ çˆ¬å–å…¬å‘Š {date_str}...")

    twse_rows = fetch_twse_attention_rows(date_obj, date_str)
    tpex_rows = fetch_tpex_attention_rows(date_obj, date_str)

    if twse_rows is None or tpex_rows is None:
        print("âŒ æŠ“å–å¤±æ•—ï¼ˆå›å‚³ Noneï¼‰ï¼Œæœ¬è¼ªä¸å¯«å…¥ç‹€æ…‹ï¼Œç•™å¾…ä¸‹æ¬¡å›è£œ")
        return None

    rows = []
    rows.extend(twse_rows)
    rows.extend(tpex_rows)

    if rows:
        print(f"âœ… æŠ“åˆ° {len(rows)} æª”")
    else:
        print("âš ï¸ ç„¡è³‡æ–™")
    return rows

def backfill_daily_logs(sh, ws_log, cal_dates, target_trade_date_obj):
    now_str = TARGET_DATE.strftime("%Y-%m-%d %H:%M:%S")
    existing_keys, date_counts = load_log_index(ws_log)
    ws_status = get_or_create_ws(sh, "çˆ¬å–ç‹€æ…‹", headers=["æ—¥æœŸ", "æŠ“åˆ°æª”æ•¸", "æœ€å¾Œæ›´æ–°æ™‚é–“"], cols=5)
    key_to_row, status_cnt = load_status_index(ws_status)
    status_is_new = (len(status_cnt) == 0)

    key_to_row, status_cnt = load_status_index(ws_status)
    window_dates = cal_dates[-MAX_BACKFILL_TRADING_DAYS:] if len(cal_dates) > MAX_BACKFILL_TRADING_DAYS else cal_dates[:]
    recent_dates = cal_dates[-VERIFY_RECENT_DAYS:] if len(cal_dates) >= VERIFY_RECENT_DAYS else cal_dates[:]
    dates_to_check = sorted(set(window_dates + recent_dates))

    rows_to_append = []
    status_updates = []

    print(f"ğŸ§© å›è£œæª¢æŸ¥ï¼šå…± {len(dates_to_check)} å€‹äº¤æ˜“æ—¥ï¼ˆå«æœ€è¿‘ {VERIFY_RECENT_DAYS} æ—¥å¼·åˆ¶é©—è­‰ï¼‰")

    for d in dates_to_check:
        d_str = d.strftime("%Y-%m-%d")
        
        if d == TARGET_DATE.date() and TARGET_DATE.time() < SAFE_CRAWL_TIME: continue

        log_cnt = int(date_counts.get(d_str, 0))
        st_cnt = status_cnt.get(d_str, None)
        need_fetch = False

        if d in recent_dates: need_fetch = True
        if (st_cnt is not None) and (log_cnt < int(st_cnt)): need_fetch = True
        if (st_cnt is None) and (log_cnt == 0): need_fetch = True
        if (st_cnt is None) and (d in window_dates): need_fetch = True

        if not need_fetch: continue

        data = get_daily_data(d)
        
        if data is None:
            print(f"âš ï¸ {d_str} æŠ“å–å¤±æ•—(None)ï¼Œè·³éä¸æ›´æ–°ç‹€æ…‹")
            continue

        official_cnt = len(data)

        for s in data:
            k = f"{s['æ—¥æœŸ']}_{s['ä»£è™Ÿ']}"
            if k not in existing_keys:
                rows_to_append.append([s['æ—¥æœŸ'], s['å¸‚å ´'], f"'{s['ä»£è™Ÿ']}", s['åç¨±'], s['è§¸çŠ¯æ¢æ¬¾']])
                existing_keys.add(k)
                date_counts[s['æ—¥æœŸ']] = date_counts.get(s['æ—¥æœŸ'], 0) + 1

        status_updates.append((d_str, official_cnt, st_cnt))

    if rows_to_append:
        print(f"ğŸ’¾ å›è£œå¯«å…¥ã€Œæ¯æ—¥ç´€éŒ„ã€ï¼š{len(rows_to_append)} ç­†")
        ws_log.append_rows(rows_to_append, value_input_option="USER_ENTERED")
    else:
        print("âœ… æ¯æ—¥ç´€éŒ„ç„¡éœ€å›è£œå¯«å…¥")

    key_to_row, status_cnt = load_status_index(ws_status)
    for d_str, official_cnt, old_st_cnt in status_updates:
        write_cnt = official_cnt
        if official_cnt == 0:
            if old_st_cnt is not None and int(old_st_cnt) > 0: write_cnt = int(old_st_cnt)
            elif int(date_counts.get(d_str, 0)) > 0: write_cnt = int(date_counts[d_str])
        upsert_status(ws_status, key_to_row, d_str, write_cnt, now_str)

def is_market_open_by_finmind(date_str):
    df = finmind_get("TaiwanStockPrice", data_id="2330", start_date=date_str, end_date=date_str)
    return not df.empty

def get_official_trading_calendar(days=60):
    end = TARGET_DATE.strftime("%Y-%m-%d")
    start = (TARGET_DATE - timedelta(days=days*2)).strftime("%Y-%m-%d")
    print("ğŸ“… ä¸‹è¼‰æ—¥æ›†...")
    df = finmind_get("TaiwanStockTradingDate", start_date=start, end_date=end)
    dates = []
    if not df.empty:
        df['date'] = pd.to_datetime(df['date']).dt.date
        dates = sorted(df['date'].tolist())
    else:
        curr = TARGET_DATE.date()
        while len(dates) < days:
            if curr.weekday() < 5: dates.append(curr)
            curr -= timedelta(days=1)
        dates = sorted(dates)
    
    today_date = TARGET_DATE.date()
    today_str = today_date.strftime("%Y-%m-%d")
    is_late_enough = TARGET_DATE.time() > SAFE_MARKET_OPEN_CHECK

    if dates and today_date > dates[-1] and today_date.weekday() < 5:
        if is_late_enough:
            print(f"âš ï¸ æ—¥æ›†ç¼ºæ¼ä»Šæ—¥ ({today_date})ï¼Œé©—è­‰é–‹å¸‚ä¸­...")
            if is_market_open_by_finmind(today_str):
                print(f"âœ… é©—è­‰æˆåŠŸ (2330æœ‰åƒ¹)ï¼Œè£œå…¥ä»Šæ—¥ã€‚")
                dates.append(today_date)
            else:
                print(f"â›” é©—è­‰å¤±æ•— (2330ç„¡åƒ¹)ï¼Œåˆ¤æ–·ç‚ºä¼‘å¸‚æˆ–è³‡æ–™æœªæ›´æ–°ï¼Œä¸è£œå…¥ã€‚")
        else:
            print(f"â³ æ™‚é–“å°šæ—©ï¼Œæš«ä¸å¼·åˆ¶è£œå…¥ä»Šæ—¥æ—¥æ›†ã€‚")

    return dates[-days:]

def get_daytrade_stats_finmind(stock_id, target_date_str):
    end = target_date_str
    start = (datetime.strptime(target_date_str, "%Y-%m-%d") - timedelta(days=15)).strftime("%Y-%m-%d")
    df_dt = finmind_get("TaiwanStockDayTrading", stock_id, start_date=start, end_date=end)
    df_p = finmind_get("TaiwanStockPrice", stock_id, start_date=start, end_date=end)
    
    if df_dt.empty or df_p.empty: return None, None
    try:
        m = pd.merge(df_p[['date', 'Trading_Volume']], df_dt[['date', 'Volume']], on='date', how='inner')
        if m.empty: return None, None
        m = m.sort_values('date')
        last = m.iloc[-1]
        td = (last['Volume']/last['Trading_Volume']*100) if last['Trading_Volume']>0 else 0
        avg = m.tail(6); sum_v = avg['Volume'].sum(); sum_t = avg['Trading_Volume'].sum()
        avg_td = (sum_v/sum_t*100) if sum_t>0 else 0
        return round(td, 2), round(avg_td, 2)
    except: return None, None

def fetch_history_data(ticker_code):
    try:
        df = yf.Ticker(ticker_code).history(period="1y", auto_adjust=False)
        if df.empty: return pd.DataFrame()
        df.index = df.index.tz_localize(None)
        return df
    except: return pd.DataFrame()

def load_precise_db_from_sheet(sh):
    try:
        ws = sh.worksheet(PARAM_SHEET_NAME)
        data = ws.get_all_records()
        db = {}
        for row in data:
            code = str(row.get('ä»£è™Ÿ', '')).strip()
            if not code: continue
            try: shares = int(str(row.get('ç™¼è¡Œè‚¡æ•¸', 1)).replace(',', ''))
            except: shares = 1
            try: offset = float(row.get('é¡è‚¡æ¼²å¹…ä¿®æ­£', 0.0))
            except: offset = 0.0
            try: turn_avg = float(row.get('åŒé¡è‚¡å¹³å‡é€±è½‰', 5.0))
            except: turn_avg = 5.0
            try: purity = float(row.get('æˆäº¤é‡ç´”åº¦', 1.0))
            except: purity = 1.0
            market = str(row.get('å¸‚å ´', 'ä¸Šå¸‚')).strip()
            db[code] = {"market": market, "shares": shares, "sector_offset": offset, "sector_turn_avg": turn_avg, "vol_purity": purity}
        return db
    except: return {}

def fetch_stock_fundamental(stock_id, ticker_code, precise_db):
    market = 'ä¸Šå¸‚'; shares = 0
    if str(stock_id) in precise_db:
        db = precise_db[str(stock_id)]
        market = db['market']; shares = db['shares']
    data = {'shares': shares, 'market_type': market, 'pe': -1, 'pb': -1}
    try:
        t = yf.Ticker(ticker_code)
        if ".TWO" in ticker_code: data['market_type'] = 'ä¸Šæ«ƒ'
        if data['shares'] <= 1:
            s = t.fast_info.get('shares', None)
            if s: data['shares'] = int(s)
        data['pe'] = t.info.get('trailingPE', t.info.get('forwardPE', 0))
        data['pb'] = t.info.get('priceToBook', 0)
        if data['pe']: data['pe'] = round(data['pe'], 2)
        if data['pb']: data['pb'] = round(data['pb'], 2)
    except: pass
    return data

def calc_pct(curr, ref):
    return ((curr - ref) / ref) * 100 if ref != 0 else 0

# âœ… [ä¿®æ­£] æ¬Šé‡èˆ‡é¢¨éšªé‚è¼¯ä¿®æ­£ï¼šå‰©2å¤©å…§ä¸€å¾‹ç´…ç‡ˆ
def calculate_full_risk(stock_id, hist_df, fund_data, est_days, dt_today_pct, dt_avg6_pct):
    res = {'risk_level': 'ä½', 'trigger_msg': '', 'curr_price': 0, 'limit_price': 0, 'gap_pct': 999.0, 'curr_vol': 0, 'limit_vol': 0, 'turnover_val': 0, 'turnover_rate': 0, 'pe': fund_data.get('pe', 0), 'pb': fund_data.get('pb', 0), 'day_trade_pct': dt_today_pct, 'is_triggered': False}
    if hist_df.empty or len(hist_df) < 7:
        if est_days <= 2: res['risk_level'] = 'é«˜'
        elif est_days <= 3: res['risk_level'] = 'ä¸­'
        return res

    curr_close = float(hist_df.iloc[-1]['Close'])
    curr_vol_shares = float(hist_df.iloc[-1]['Volume'])
    curr_vol_lots = int(curr_vol_shares / UNIT_LOT)
    shares = fund_data.get('shares', 1)
    if shares > 1: turnover = (curr_vol_shares / shares) * 100
    else: turnover = -1.0
    turnover_val_money = curr_close * curr_vol_shares

    res['curr_price'] = round(curr_close, 2)
    res['curr_vol'] = curr_vol_lots
    res['turnover_rate'] = round(turnover, 2)
    res['turnover_val'] = round(turnover_val_money / 100000000, 2)

    triggers = []
    if curr_close < 5: return res

    window_7 = hist_df.tail(7)
    ref_6 = float(window_7.iloc[0]['Close'])
    rise_6 = calc_pct(curr_close, ref_6)
    price_diff_6 = abs(curr_close - ref_6)

    cond_1 = rise_6 > 32
    cond_2 = (rise_6 > 25) and (price_diff_6 >= 50)
    if cond_1: triggers.append(f"ã€ç¬¬ä¸€æ¬¾ã€‘6æ—¥æ¼²{rise_6:.1f}%(>32%)")
    elif cond_2: triggers.append(f"ã€ç¬¬ä¸€æ¬¾ã€‘6æ—¥æ¼²{rise_6:.1f}%ä¸”åƒ¹å·®{price_diff_6:.0f}å…ƒ")

    limit_p = ref_6 * 1.32
    if cond_2: limit_p = min(limit_p, ref_6 * 1.25)
    res['limit_price'] = round(limit_p, 2)
    res['gap_pct'] = round(((limit_p - curr_close)/curr_close)*100, 1)

    if len(hist_df)>=31 and calc_pct(curr_close, float(hist_df.iloc[-31]['Close'])) > 100: triggers.append("ã€ç¬¬äºŒæ¬¾ã€‘30æ—¥æ¼²>100%")
    if len(hist_df)>=61 and calc_pct(curr_close, float(hist_df.iloc[-61]['Close'])) > 130: triggers.append("ã€ç¬¬äºŒæ¬¾ã€‘60æ—¥æ¼²>130%")
    if len(hist_df)>=91 and calc_pct(curr_close, float(hist_df.iloc[-91]['Close'])) > 160: triggers.append("ã€ç¬¬äºŒæ¬¾ã€‘90æ—¥æ¼²>160%")

    if len(hist_df) >= 61:
        avg_vol_60 = hist_df['Volume'].iloc[-61:-1].mean()
        if avg_vol_60 > 0:
            vol_ratio = curr_vol_shares / avg_vol_60
            res['limit_vol'] = int(avg_vol_60 * 5 / 1000)
            if turnover >= 0.1 and curr_vol_lots >= 500:
                if rise_6 > 25 and vol_ratio > 5: triggers.append(f"ã€ç¬¬ä¸‰æ¬¾ã€‘æ¼²{rise_6:.0f}%+é‡{vol_ratio:.1f}å€")

    if turnover > 10 and rise_6 > 25: triggers.append(f"ã€ç¬¬å››æ¬¾ã€‘æ¼²{rise_6:.0f}%+è½‰{turnover:.0f}%")

    if len(hist_df) >= 61:
        avg_vol_60 = hist_df['Volume'].iloc[-61:-1].mean()
        avg_vol_6 = hist_df['Volume'].iloc[-6:].mean()
        is_exclude = (turnover < 0.1) or (curr_vol_lots < 500) or (turnover_val_money < 30000000)
        if not is_exclude and avg_vol_60 > 0:
            r1 = avg_vol_6 / avg_vol_60
            r2 = curr_vol_shares / avg_vol_60
            if r1 > 5: triggers.append(f"ã€ç¬¬ä¹æ¬¾ã€‘6æ—¥å‡é‡æ”¾å¤§{r1:.1f}å€")
            if r2 > 5: triggers.append(f"ã€ç¬¬ä¹æ¬¾ã€‘ç•¶æ—¥é‡æ”¾å¤§{r2:.1f}å€")

    if turnover > 0 and turnover_val_money >= 500000000:
        acc_turn = (hist_df['Volume'].iloc[-6:].sum() / shares) * 100
        if acc_turn > 50 and turnover > 10: triggers.append(f"ã€ç¬¬åæ¬¾ã€‘ç´¯è½‰{acc_turn:.0f}%")

    if len(hist_df) >= 6:
        gap = hist_df.iloc[-6:]['High'].max() - hist_df.iloc[-6:]['Low'].min()
        threshold = 100 + (int((curr_close - 500)/500)+1)*25 if curr_close >= 500 else 100
        if gap >= threshold: triggers.append(f"ã€ç¬¬åä¸€æ¬¾ã€‘6æ—¥åƒ¹å·®{gap:.0f}å…ƒ(>é–€æª»{threshold})")

    pending_msg = ""
    if dt_today_pct is None or dt_avg6_pct is None:
        pending_msg = "(ç•¶æ²–ç‡å¾…å…¬å¸ƒ)"
    else:
        dt_vol_est = curr_vol_shares * (dt_today_pct / 100.0)
        dt_vol_lots = dt_vol_est / 1000
        is_exclude = (turnover < 5) or (turnover_val_money < 500000000) or (dt_vol_lots < 5000)
        if not is_exclude:
            if dt_avg6_pct > 60 and dt_today_pct > 60:
                triggers.append(f"ã€ç¬¬åä¸‰æ¬¾ã€‘ç•¶æ²–{dt_today_pct}%(6æ—¥{dt_avg6_pct}%)")

    if triggers:
        res['is_triggered'] = True
        res['risk_level'] = 'é«˜'
        res['trigger_msg'] = "ä¸”".join(triggers) + (f" {pending_msg}" if pending_msg else "")
    else:
        res['trigger_msg'] = pending_msg
        
    # âœ… é—œéµä¿®æ­£ï¼šå‰©é¤˜å¤©æ•¸ <= 2 å¼·åˆ¶å‡ç´šç‚ºé«˜é¢¨éšª(ç´…ç‡ˆ)
    if est_days <= 2:
        res['risk_level'] = 'é«˜'
    elif est_days <= 3 and res['risk_level'] != 'é«˜':
        res['risk_level'] = 'ä¸­'
    
    return res

def check_jail_trigger_now(status_list, clause_list):
    status_list = list(status_list); clause_list = list(clause_list)
    if len(status_list) < 30:
        pad = 30 - len(status_list)
        status_list = [0]*pad + status_list
        clause_list = [""]*pad + clause_list

    c1_streak = 0
    for c in clause_list[-3:]:
        if 1 in parse_clause_ids_strict(c): c1_streak += 1

    v5 = 0; v10 = 0; v30 = 0
    total = len(status_list)
    for i in range(30):
        idx = total - 1 - i
        if idx < 0: break
        if status_list[idx] == 1:
            ids = parse_clause_ids_strict(clause_list[idx])
            if is_valid_accumulation_day(ids):
                if i < 5: v5 += 1
                if i < 10: v10 += 1
                v30 += 1

    reasons = []
    if c1_streak == 3: reasons.append("å·²è§¸ç™¼(é€£3ç¬¬ä¸€æ¬¾)")
    if v5 == 5: reasons.append("å·²è§¸ç™¼(é€£5)")
    if v10 >= 6: reasons.append(f"å·²è§¸ç™¼(10æ—¥{v10}æ¬¡)")
    if v30 >= 12: reasons.append(f"å·²è§¸ç™¼(30æ—¥{v30}æ¬¡)")
    return (len(reasons) > 0), " | ".join(reasons)

def simulate_days_to_jail_strict(status_list, clause_list, *, stock_id=None, target_date=None, jail_map=None, enable_safe_filter=True):
    if stock_id and target_date and jail_map and is_in_jail(stock_id, target_date, jail_map):
        return 0, "è™•ç½®ä¸­"

    trigger_now, reason_now = check_jail_trigger_now(status_list, clause_list)
    if trigger_now:
        return 0, reason_now.replace("å·²è§¸ç™¼", "å·²é”æ¨™ï¼Œæ¬¡ä¸€ç‡Ÿæ¥­æ—¥è™•ç½®")

    if enable_safe_filter:
        recent_valid_10 = 0
        check_len = min(len(status_list), 10)
        if check_len > 0:
            for b, c in zip(status_list[-check_len:], clause_list[-check_len:]):
                if b == 1 and is_valid_accumulation_day(parse_clause_ids_strict(c)):
                    recent_valid_10 += 1
        if recent_valid_10 == 0: return 99, "X"

    status_list = list(status_list); clause_list = list(clause_list)
    if len(status_list) < 30:
        pad = 30 - len(status_list)
        status_list = [0]*pad + status_list
        clause_list = [""]*pad + clause_list

    days = 0
    while days < 10:
        days += 1
        status_list.append(1); clause_list.append("ç¬¬1æ¬¾")
        
        c1_streak = 0
        for c in clause_list[-3:]:
            if 1 in parse_clause_ids_strict(c): c1_streak += 1

        v5 = 0; v10 = 0; v30 = 0
        total = len(status_list)
        for i in range(30):
            idx = total - 1 - i
            if idx < 0: break
            if status_list[idx] == 1:
                ids = parse_clause_ids_strict(clause_list[idx])
                if is_valid_accumulation_day(ids):
                    if i < 5: v5 += 1
                    if i < 10: v10 += 1
                    v30 += 1
        
        reasons = []
        if c1_streak == 3: reasons.append(f"å†{days}å¤©è™•ç½®")
        if v5 == 5: reasons.append(f"å†{days}å¤©è™•ç½®(é€£5)")
        if v10 >= 6: reasons.append(f"å†{days}å¤©è™•ç½®(10æ—¥{v10}æ¬¡)")
        if v30 >= 12: reasons.append(f"å†{days}å¤©è™•ç½®(30æ—¥{v30}æ¬¡)")

        if reasons:
            return days, " | ".join(reasons)
            
    return 99, ""

# ============================
# Main
# ============================
def main():
    sh, _ = connect_google_sheets()
    if not sh: return

    update_market_monitoring_log(sh)

    cal_dates = get_official_trading_calendar(240)
    
    # âœ… [ä¿®æ­£] main() ä¿®æ­£ T-2 å›æœ” Bug
    target_trade_date_obj = cal_dates[-1]
    is_today_trade = (target_trade_date_obj == TARGET_DATE.date())

    # åªæœ‰ã€Œæ—¥æ›†å·²åŒ…å«ä»Šå¤©ã€ä¸”ã€Œç¾åœ¨ < 17:30ã€æ‰é€€å› T-1
    if is_today_trade and (not IS_AFTER_SAFE) and len(cal_dates) >= 2:
        print(f"â³ ç¾åœ¨æ™‚é–“ {TARGET_DATE.strftime('%H:%M')} æ—©æ–¼ {SAFE_CRAWL_TIME}ï¼Œä¸”æ—¥æ›†åŒ…å«ä»Šæ—¥ï¼Œåˆ‡æ›ç‚º T-1 æ¨¡å¼ã€‚")
        target_trade_date_obj = cal_dates[-2]

    target_date_str = target_trade_date_obj.strftime("%Y-%m-%d")
    print(f"ğŸ“… æœ€çµ‚é–å®šé‹ç®—æ—¥æœŸ: {target_date_str}")

    ws_log = get_or_create_ws(sh, "æ¯æ—¥ç´€éŒ„", headers=['æ—¥æœŸ','å¸‚å ´','ä»£è™Ÿ','åç¨±','è§¸çŠ¯æ¢æ¬¾'])
    
    # âœ… åŸ·è¡Œå›è£œ (åŒ…å«æª¢æŸ¥ç‹€æ…‹è¡¨ç¼ºå¤±)
    backfill_daily_logs(sh, ws_log, cal_dates, target_trade_date_obj)

    print("ğŸ“Š è®€å–æ­·å² Log...")
    log_data = ws_log.get_all_records()
    df_log = pd.DataFrame(log_data)
    if not df_log.empty:
        df_log['ä»£è™Ÿ'] = df_log['ä»£è™Ÿ'].astype(str).str.strip().str.replace("'", "")
        # âœ… [ä¿®æ­£] å¼·åˆ¶æ—¥æœŸæ¨™æº–åŒ– (YYYY-MM-DD)ï¼Œè§£æ±º Google Sheets æ ¼å¼æ··äº‚å•é¡Œ
        df_log['æ—¥æœŸ'] = pd.to_datetime(df_log['æ—¥æœŸ'], errors='coerce').dt.strftime("%Y-%m-%d")
        df_log = df_log[df_log['æ—¥æœŸ'].notna()]

    clause_map = {}
    for _, r in df_log.iterrows():
        key = (str(r['ä»£è™Ÿ']), str(r['æ—¥æœŸ']))
        clause_map[key] = merge_clause_text(clause_map.get(key,""), str(r['è§¸çŠ¯æ¢æ¬¾']))

    # âœ… [ä¿®æ­£] è™•ç½®åå–®èˆ‡æƒæå€é–“çµ±ä¸€èª¿æ•´ç‚º 90 å¤©
    jail_map = get_jail_map(target_trade_date_obj - timedelta(days=90), target_trade_date_obj)
    exclude_map = build_exclude_map(cal_dates, jail_map)

    start_dt_str = cal_dates[-90].strftime("%Y-%m-%d")
    df_recent = df_log[df_log['æ—¥æœŸ'] >= start_dt_str]
    target_stocks = df_recent['ä»£è™Ÿ'].unique()
    
    precise_db = load_precise_db_from_sheet(sh)
    rows_stats = []
    
    print(f"ğŸ” æƒæ {len(target_stocks)} æª”è‚¡ç¥¨...")
    for idx, code in enumerate(target_stocks):
        code = str(code).strip()
        name = df_log[df_log['ä»£è™Ÿ']==code]['åç¨±'].iloc[-1] if not df_log[df_log['ä»£è™Ÿ']==code].empty else "æœªçŸ¥"
        
        db_info = precise_db.get(code, {})
        m_type = str(db_info.get('market', 'ä¸Šå¸‚')).upper()
        suffix = '.TWO' if any(k in m_type for k in ['ä¸Šæ«ƒ', 'TWO', 'TPEX', 'OTC']) else '.TW'
        ticker_code = f"{code}{suffix}"

        stock_calendar = get_last_n_non_jail_trade_dates(code, cal_dates, jail_map, exclude_map, 30)
        
        bits = []; clauses = []
        for d in stock_calendar:
            c = clause_map.get((code, d.strftime("%Y-%m-%d")), "")
            if is_excluded(code, d, exclude_map):
                bits.append(0); clauses.append(c); continue
            if c: bits.append(1); clauses.append(c)
            else: bits.append(0); clauses.append("")

        # âœ… [ä¿®æ­£] å¼·åˆ¶ enable_safe_filter=False (å‰›å‡ºé—œä¸è¢«æ¿¾æ‰)
        est_days, reason = simulate_days_to_jail_strict(
            bits, clauses, 
            stock_id=code, 
            target_date=target_trade_date_obj, 
            jail_map=jail_map,
            enable_safe_filter=False
        )
        
        latest_ids = parse_clause_ids_strict(clauses[-1] if clauses else "")
        is_special_risk = is_special_risk_day(latest_ids)
        is_clause_13 = False
        for c in clauses:
            if 13 in parse_clause_ids_strict(c):
                is_clause_13 = True
                break

        est_days_int = 99
        est_days_display = "X"
        reason_display = ""

        if reason == "X":
            est_days_int = 99
            est_days_display = "X"
            if is_special_risk:
                reason_display = "ç±Œç¢¼ç•°å¸¸(äººå·¥å¯©æ ¸é¢¨éšª)"
                if is_clause_13: reason_display += " + åˆ‘æœŸå¯èƒ½å»¶é•·"
        elif est_days == 0:
            est_days_int = 0
            est_days_display = "0"
            reason_display = reason
        else:
            est_days_int = int(est_days)
            est_days_display = str(est_days_int)
            reason_display = reason
            if is_special_risk:
                reason_display += " | âš ï¸ç•™æ„äººå·¥è™•ç½®é¢¨éšª"
            if is_clause_13:
                reason_display += " (è‹¥é€²è™•ç½®å°‡é—œ12å¤©)"

        hist = fetch_history_data(ticker_code)
        if hist.empty:
            alt_s = '.TWO' if suffix=='.TW' else '.TW'
            hist = fetch_history_data(f"{code}{alt_s}")
            if not hist.empty: ticker_code = f"{code}{alt_s}"

        fund = fetch_stock_fundamental(code, ticker_code, precise_db)
        
        # âœ… ç•¶æ²–ç‡æŠ“å–åˆ¤æ–·ï¼šåªæœ‰éäº† 21:00 æ‰æŠ“ï¼Œå¦å‰‡çµ¦ None
        dt_today, dt_avg6 = None, None
        if IS_AFTER_DAYTRADE:
            dt_today, dt_avg6 = get_daytrade_stats_finmind(code, target_date_str)

        risk = calculate_full_risk(code, hist, fund, est_days_int, dt_today, dt_avg6)

        # streak
        valid_bits = [1 if b==1 and is_valid_accumulation_day(parse_clause_ids_strict(c)) else 0 for b,c in zip(bits, clauses)]
        streak = 0
        for v in reversed(valid_bits):
            if v: streak+=1
            else: break
            
        status_30 = "".join(map(str, valid_bits)).zfill(30)
        
        def safe(v):
            if v is None: return ""
            try: 
                if np.isnan(v): return ""
            except: pass
            return str(v)

        row = [
            f"'{code}", name, safe(streak), safe(sum(valid_bits)), safe(sum(valid_bits[-10:])),
            stock_calendar[-1].strftime("%Y-%m-%d") if stock_calendar else "",
            f"'{status_30}", f"'{status_30[-10:]}", est_days_display, safe(reason_display),
            safe(risk['risk_level']), safe(risk['trigger_msg']),
            safe(risk['curr_price']), safe(risk['limit_price']), safe(risk['gap_pct']),
            safe(risk['curr_vol']), safe(risk['limit_vol']), safe(risk['turnover_val']),
            safe(risk['turnover_rate']), safe(risk['pe']), safe(risk['pb']), safe(risk['day_trade_pct'])
        ]
        rows_stats.append(row)
        if (idx+1)%10==0: time.sleep(1)

    if rows_stats:
        print("ğŸ’¾ æ›´æ–°çµ±è¨ˆè¡¨...")
        ws_stats = get_or_create_ws(sh, "è¿‘30æ—¥ç†±é–€çµ±è¨ˆ", headers=STATS_HEADERS)
        ws_stats.clear()
        ws_stats.append_row(STATS_HEADERS, value_input_option='USER_ENTERED')
        ws_stats.append_rows(rows_stats, value_input_option='USER_ENTERED')
        print("âœ… å®Œæˆ")

if __name__ == "__main__":
    main()
